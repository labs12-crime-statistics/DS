{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pip in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (19.1.1)\n",
      "Requirement already satisfied: python-decouple in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (3.1)\n",
      "Requirement already satisfied: geoalchemy2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (0.6.2)\n",
      "Requirement already satisfied: SQLAlchemy>=0.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from geoalchemy2) (1.2.11)\n",
      "Requirement already satisfied: shapely in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (1.6.4.post2)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (1.1.0)\n",
      "Requirement already satisfied: keras in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (2.2.4)\n",
      "Requirement already satisfied: h5py in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from keras) (2.8.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from keras) (1.11.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from keras) (1.15.4)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from keras) (3.12)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from keras) (1.0.7)\n",
      "Requirement already satisfied: scipy>=0.14 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from keras) (1.1.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from keras) (1.0.9)\n",
      "Collecting tensorflow\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/77/63/a9fa76de8dffe7455304c4ed635be4aa9c0bacef6e0633d87d5f54530c5c/tensorflow-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (92.5MB)\n",
      "\u001b[K     |████████████████████████████████| 92.5MB 62.8MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: keras-applications>=1.0.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.0.7)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (3.6.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.0.9)\n",
      "Collecting gast>=0.2.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
      "Collecting astor>=0.6.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/35/6b/11530768cac581a12952a2aad00e1526b89d242d0b9f59534ef6e6a1752f/astor-0.7.1-py2.py3-none-any.whl\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Collecting grpcio>=1.8.6 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0a/9d/8bd5d0e516b196f59f1c4439b424b8d4fa62d492a4b531aae322d2d82a7b/grpcio-1.20.1-cp36-cp36m-manylinux1_x86_64.whl (2.1MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2MB 33.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard<1.14.0,>=1.13.0 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
      "\u001b[K     |████████████████████████████████| 3.2MB 40.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-estimator<1.14.0rc0,>=1.13.0 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
      "\u001b[K     |████████████████████████████████| 368kB 55.7MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.11.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (0.31.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.15.4)\n",
      "Collecting absl-py>=0.1.6 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/3f/9b0355080b81b15ba6a9ffcf1f5ea39e307a2778b2f2dc8694724e8abd5b/absl-py-0.7.1.tar.gz (99kB)\n",
      "\u001b[K     |████████████████████████████████| 102kB 42.8MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: h5py in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from keras-applications>=1.0.6->tensorflow) (2.8.0)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from protobuf>=3.6.1->tensorflow) (39.1.0)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<1.14.0,>=1.13.0->tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/e4/d8c18f2555add57ff21bf25af36d827145896a07607486cc79a2aea641af/Markdown-3.1-py2.py3-none-any.whl (87kB)\n",
      "\u001b[K     |████████████████████████████████| 92kB 38.0MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow) (0.14.1)\n",
      "Requirement already satisfied: mock>=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: pbr>=0.11 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from mock>=2.0.0->tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow) (5.1.3)\n",
      "Building wheels for collected packages: gast, termcolor, absl-py\n",
      "  Building wheel for gast (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ec2-user/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ec2-user/.cache/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
      "  Building wheel for absl-py (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ec2-user/.cache/pip/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48\n",
      "Successfully built gast termcolor absl-py\n",
      "Installing collected packages: gast, astor, termcolor, grpcio, markdown, absl-py, tensorboard, tensorflow-estimator, tensorflow\n",
      "Successfully installed absl-py-0.7.1 astor-0.7.1 gast-0.2.2 grpcio-1.20.1 markdown-3.1 tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0 termcolor-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install python-decouple\n",
    "!pip install geoalchemy2\n",
    "!pip install shapely\n",
    "!pip install scipy\n",
    "!pip install keras\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine, func, text\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from decouple import config\n",
    "from shapely import wkb, wkt\n",
    "from shapely.geometry import Point\n",
    "from geoalchemy2.shape import to_shape \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# ----------- TODO: Issues with importing imbalance-learn library\n",
    "# from imblearn.over_sampling import RandomOverSampler\n",
    "# from imblearn.under_sampling import NearMiss\n",
    "# from imblearn.under_sampling import (RandomUnderSampler,\n",
    "#                                      ClusterCentroids,\n",
    "#                                      TomekLinks,\n",
    "#                                      NeighbourhoodCleaningRule,\n",
    "#                                      NearMiss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Contains models for DB.\"\"\"\n",
    "\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy import Column, BigInteger, Integer, String, DateTime, ForeignKey, Float\n",
    "from sqlalchemy.orm import relationship\n",
    "from geoalchemy2 import Geometry\n",
    "\n",
    "\n",
    "BASE = declarative_base()\n",
    "\n",
    "\n",
    "class City(BASE):\n",
    "    \"\"\"City model for DB. Has information of cities.\"\"\"\n",
    "    __tablename__ = 'city'\n",
    "    id            = Column(BigInteger, primary_key=True)\n",
    "    city          = Column(String, unique=False, nullable=False)\n",
    "    state         = Column(String, unique=False, nullable=True)\n",
    "    country       = Column(String, unique=False, nullable=False)\n",
    "    location      = Column(Geometry(geometry_type='POINT'), nullable=False)\n",
    "    blocks        = relationship(\"Blocks\", back_populates=\"city\")\n",
    "    zipcodes      = relationship(\"ZipcodeGeom\", back_populates=\"city\")\n",
    "    incidents     = relationship(\"Incident\", back_populates=\"city\")\n",
    "\n",
    "\n",
    "class Blocks(BASE):\n",
    "    \"\"\"Block model for DB. Has information of city blocks for a related city\n",
    "        id.\"\"\"\n",
    "    __tablename__ = 'block'\n",
    "    id            = Column(BigInteger, primary_key=True)\n",
    "    cityid        = Column(BigInteger, ForeignKey('city.id'), nullable=False)\n",
    "    shape         = Column(Geometry(geometry_type='MULTIPOLYGON'), nullable=False)\n",
    "    population    = Column(Integer, nullable=False)\n",
    "    city          = relationship(\"City\", back_populates=\"blocks\")\n",
    "    incidents     = relationship(\"Incident\", back_populates=\"block\")\n",
    "\n",
    "class ZipcodeGeom(BASE):\n",
    "    \"\"\"Zipcode geometry model for DB. Has information of zipcodes and related\n",
    "        city id.\"\"\"\n",
    "    __tablename__ = 'zipcodegeom'\n",
    "    id            = Column(BigInteger, primary_key=True)\n",
    "    cityid        = Column(BigInteger, ForeignKey('city.id'), nullable=False)\n",
    "    zipcode       = Column(String, nullable=False, unique=True)\n",
    "    shape         = Column(Geometry(geometry_type='MULTIPOLYGON'), nullable=False)\n",
    "    city          = relationship(\"City\", back_populates=\"zipcodes\")\n",
    "\n",
    "class Incident(BASE):\n",
    "    \"\"\"Incident model for DB. Has information of a specific crime, including\n",
    "        where it took place, when it took place, and the type of crime that\n",
    "        occurred.\"\"\"\n",
    "    __tablename__ = 'incident'\n",
    "    id            = Column(BigInteger, primary_key=True)\n",
    "    crimetypeid   = Column(BigInteger, ForeignKey('crimetype.id'), nullable=False)\n",
    "    locdescid     = Column(BigInteger, ForeignKey('locdesctype.id'), nullable=False)\n",
    "    cityid        = Column(BigInteger, ForeignKey('city.id'), nullable=False)\n",
    "    blockid       = Column(BigInteger, ForeignKey('block.id'), nullable=False)\n",
    "    location      = Column(Geometry(geometry_type='POINT'), nullable=False)\n",
    "    datetime      = Column(DateTime, nullable=False)\n",
    "    hour          = Column(Integer, nullable=False)\n",
    "    dow           = Column(Integer, nullable=False)\n",
    "    month         = Column(Integer, nullable=False)\n",
    "    year          = Column(Integer, nullable=False)\n",
    "    city          = relationship(\"City\", back_populates=\"incidents\")\n",
    "    block         = relationship(\"Blocks\", back_populates=\"incidents\")\n",
    "    crimetype     = relationship(\"CrimeType\", back_populates=\"incidents\")\n",
    "    locationdesc  = relationship(\"LocationDescriptionType\", back_populates=\"incidents\")\n",
    "\n",
    "class CrimeType(BASE):\n",
    "    \"\"\"CrimeType model for DB. Has information of the types of crime, including\n",
    "        a general description and the numerical severity of the crime.\"\"\"\n",
    "    __tablename__ = 'crimetype'\n",
    "    id            = Column(BigInteger, primary_key=True)\n",
    "    category      = Column(String, unique=True, nullable=False)\n",
    "    severity      = Column(Integer, nullable=False)\n",
    "    incidents     = relationship(\"Incident\", back_populates=\"crimetype\")\n",
    "\n",
    "\n",
    "class LocationDescriptionType(BASE):\n",
    "    \"\"\"Location description model for DB. Has information on the type of\n",
    "        location that the crime took place.\"\"\"\n",
    "    __tablename__ = 'locdesctype'\n",
    "    id            = Column(BigInteger, primary_key=True)\n",
    "    key1          = Column(String, nullable=False)\n",
    "    key2          = Column(String, nullable=False)\n",
    "    key3          = Column(String, nullable=False)\n",
    "    incidents     = relationship(\"Incident\", back_populates=\"locationdesc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetData(object):\n",
    "    def go(self, SESSION, start_year, end_year):\n",
    "        SQL_QUERY = \\\n",
    "            f'''\n",
    "                SELECT\n",
    "                    incident.blockid,\n",
    "                    incident.year,\n",
    "                    incident.month,\n",
    "                    incident.dow,\n",
    "                    incident.hour,\n",
    "                    SUM(crimetype.severity)/AVG(block.population) AS severity\n",
    "                FROM incident\n",
    "                INNER JOIN block ON incident.blockid = block.id\n",
    "                INNER JOIN crimetype ON incident.crimetypeid = crimetype.id\n",
    "                    AND block.population > 0\n",
    "                    AND incident.cityid = 1\n",
    "                    AND incident.year >= {start_year}\n",
    "                    AND incident.year <= {end_year}\n",
    "                GROUP BY\n",
    "                    incident.blockid,\n",
    "                    incident.year,\n",
    "                    incident.month,\n",
    "                    incident.dow,\n",
    "                    incident.hour\n",
    "            '''\n",
    "        return SESSION.execute(text(SQL_QUERY)).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data, start_year, end_year, map_risk):\n",
    "\n",
    "    NUM_BLOCKIDS = 801\n",
    "    \n",
    "    X = np.zeros((NUM_BLOCKIDS, 24, 7*24))\n",
    "    y = np.zeros((NUM_BLOCKIDS, 12, 7*24))\n",
    "    blockid_dict = {}\n",
    "\n",
    "    # Create random array (BLOCKIDS) from 1-801 inclusive\n",
    "    # of length NUM_BLOCKIDS\n",
    "    BLOCKIDS = random.choices(list(range(1,802)), k=NUM_BLOCKIDS)\n",
    "    \n",
    "    for ind, blockid in enumerate(BLOCKIDS):\n",
    "        blockid_dict[blockid] = ind\n",
    "\n",
    "    # records is the list of rows we get from the query with this order:\n",
    "    #   blockid, year, month, dow, hour, risk\n",
    "    #   month is from 1 - 12\n",
    "\n",
    "    for r in data:\n",
    "        if r[0] in blockid_dict:\n",
    "            if r[1] == end_year:\n",
    "                # index into array  0-based month\n",
    "                # vvvvvvvvvvvvvvvv    vvvvvv\n",
    "                y[blockid_dict[r[0]], r[2]-1, 24*r[3]+r[4]] = map_risk(float(r[5]))\n",
    "                #                             ^^^^^^^^^^^^^   ^^^^\n",
    "                #                             hours since     risk\n",
    "                #                             beginning of\n",
    "                #                             week\n",
    "            else:\n",
    "                # index into array    year 0.....1   month   \n",
    "                # vvvvvvvvvvvvvvvv    vvvvvvvvvvvvv  vvvvvv\n",
    "                X[blockid_dict[r[0]], 12*(r[1]-start_year)+r[2]-1, 24*r[3]+r[4]] = map_risk(float(r[5]))\n",
    "                #                                                  ^^^^^^^^^^^^^   ^^^^\n",
    "                #                                                  hours since     risk\n",
    "                #                                                  beginning of\n",
    "                #                                                  week                \n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def session_scope():\n",
    "    \"\"\"Provide a transactional scope around a series of operations.\"\"\"\n",
    "\n",
    "    DB_URI  = config('DB_URI')\n",
    "    ENGINE  = create_engine(DB_URI)\n",
    "    Session = sessionmaker(bind=ENGINE)\n",
    "    SESSION = Session()\n",
    "    \n",
    "    try:\n",
    "        yield SESSION\n",
    "        SESSION.commit()\n",
    "    except:\n",
    "        SESSION.rollback()\n",
    "        raise\n",
    "    finally:\n",
    "        SESSION.close()\n",
    "\n",
    "\n",
    "def ready_data(training_start_year, training_end_year,\n",
    "               testing_start_year, testing_end_year,\n",
    "               map_risk):\n",
    "    with session_scope() as session:\n",
    "        training_data = GetData().go(session,\n",
    "                                     training_start_year,\n",
    "                                     training_end_year)\n",
    "        testing_data = GetData().go(session,\n",
    "                                     testing_start_year,\n",
    "                                     testing_end_year)\n",
    "        \n",
    "        X_train, y_train = process_data(training_data,\n",
    "                                        training_start_year, \n",
    "                                        training_end_year,\n",
    "                                        map_risk)\n",
    "        X_test, y_test = process_data(testing_data,\n",
    "                                      testing_start_year, \n",
    "                                      testing_end_year,\n",
    "                                      map_risk)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "def map_risk_1(risk):\n",
    "    return np.power(risk * 1e5, 2)\n",
    "\n",
    "def map_risk_0(risk):\n",
    "    return risk\n",
    "\n",
    "def map_risk_2(risk):\n",
    "    return np.power(risk * 1e5, 3)\n",
    "\n",
    "def map_risk_3(risk):\n",
    "    return np.power(risk * 1e5, 4)\n",
    "\n",
    "map_risk = [map_risk_1, map_risk_2, map_risk_3]  # , map_risk_1, map_risk_2, map_risk_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "801 801 801 801\n",
      "CPU times: user 2.9 s, sys: 343 ms, total: 3.24 s\n",
      "Wall time: 9.83 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train, X_test, y_train, y_test = ready_data(2015, 2017, 2016, 2018, map_risk_0)\n",
    "print(len(X_train), len(y_train), len(X_test), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((801, 24, 168), (801, 12, 168), (801, 24, 168), (801, 12, 168))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_saved, y_train_saved, X_test_saved, y_test_saved = X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(data=X_train, columns=['risk_past'])\n",
    "y_train = pd.DataFrame(data=y_train, columns=['risk_future'])\n",
    "X_test = pd.DataFrame(data=X_test, columns=['risk_past'])\n",
    "y_test = pd.DataFrame(data=y_test, columns=['risk_future'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_output(y, y_pred, dataset_type):\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    plt.plot(np.arange(len(y.flatten())),\n",
    "                 y.flatten(), color='blue');\n",
    "    plt.plot(np.arange(len(y_pred.flatten())),\n",
    "                 y_pred.flatten(), color='red');\n",
    "    plt.xlabel('Hour since beginning of data', fontsize=16)\n",
    "    plt.ylabel('Risk', fontsize=18)\n",
    "    plt.title(dataset_type + ' dataset', fontsize=18)\n",
    "    plt.legend(labels=['risk', 'predicted risk'], prop={'size': 20})\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_validation_loss(epochs, history):\n",
    "    plt.plot(range(1, epochs), history.history['loss'])\n",
    "    plt.plot(range(1, epochs), history.history['val_loss'])\n",
    "    plt.title('model train vs validation loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Reshape, Dropout\n",
    "from keras.optimizers import Adam, SGD, RMSprop, Adagrad, Adadelta, Adamax, Nadam\n",
    "import keras.backend as K\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "def perc_error(y_true, y_pred):\n",
    "    return 100.0 * K.mean((y_true - y_pred) / y_true)\n",
    "\n",
    "def create_model(learn_rate=0.01, \n",
    "                 momentum=0,\n",
    "                 opt_name='Adam',\n",
    "                 init_mode='uniform',\n",
    "                 activation='relu'\n",
    "                ):\n",
    "    data_dim    = 7 * 24   # All values in each hour of the week\n",
    "                           # averaged over each day for all weeks\n",
    "                           # of the month\n",
    "    timesteps   = 2 * 12   # Summed per month\n",
    "    batch_size  = 64\n",
    "    num_outputs = 7 * 24 * 12\n",
    "    \n",
    "    # expected input data shape: (batch_size, timesteps, data_dim)\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(1024, input_shape=(24,168), activation='relu'))\n",
    "    model.add(Dense(12*168, kernel_initializer='zero', activation='relu'))\n",
    "    model.add(Reshape((12,168)))\n",
    "    \n",
    "    optimizer = { \n",
    "        'Adam':     Adam(lr=learn_rate),\n",
    "    }\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer[opt_name])\n",
    "\n",
    "    return model\n",
    "\n",
    "def predict(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    # GridSearchCV's fit method requires X which is 2D, y which is 1D.\n",
    "    # This is a problem for us since our X and y are 3D.\n",
    "    # Instead of GridSearchCV, we will create our own loop to\n",
    "    # search through the grid.\n",
    "    # \n",
    "    batch_size = 64\n",
    "    for epochs in range(5, 6, 5):\n",
    "        for lr in [0.0001, 0.001, 0.01, 0.1]:\n",
    "            for opt_name in ['Adam']:\n",
    "                for init_mode in ['zero']:\n",
    "                    for activation in ['relu']:\n",
    "\n",
    "                        print('>'*80)\n",
    "                        print('epochs:', epochs, '   lr:', lr, \\\n",
    "                              '   opt:', opt_name, \\\n",
    "                              '   init:', init_mode, \\\n",
    "                              '   act:', 'relu')\n",
    "                        print('>'*80)\n",
    "\n",
    "                        model = create_model()\n",
    "\n",
    "                        model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "                        history = model.fit(X_train, y_train,\n",
    "                                            batch_size=batch_size, \n",
    "                                            epochs=epochs,\n",
    "                                            validation_data=(X_test, y_test))\n",
    "\n",
    "                        mse = model.evaluate(X_test, y_test,\n",
    "                                             batch_size=batch_size)\n",
    "                        print('<'*80)\n",
    "                        print('epochs:', epochs, '   lr:', lr, \\\n",
    "                              '   opt:', opt_name, \\\n",
    "                              '   init:', init_mode, \\\n",
    "                              '   act:', 'relu', \\\n",
    "                              ' Test MSE:', mse \\\n",
    "                        )\n",
    "                        print('<'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epochs: 5    lr: 0.0001    opt: Adam    init: zero    act: relu\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 801 samples, validate on 801 samples\n",
      "Epoch 1/5\n",
      "801/801 [==============================] - 6s 7ms/step - loss: 2.6612e-06 - val_loss: 8.0647e-07\n",
      "Epoch 2/5\n",
      "801/801 [==============================] - 4s 6ms/step - loss: 2.6612e-06 - val_loss: 8.0647e-07\n",
      "Epoch 3/5\n",
      "801/801 [==============================] - 4s 6ms/step - loss: 2.6612e-06 - val_loss: 8.0647e-07\n",
      "Epoch 4/5\n",
      "801/801 [==============================] - 4s 6ms/step - loss: 2.6612e-06 - val_loss: 8.0647e-07\n",
      "Epoch 5/5\n",
      "801/801 [==============================] - 4s 6ms/step - loss: 2.6612e-06 - val_loss: 8.0647e-07\n",
      "801/801 [==============================] - 1s 1ms/step\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "epochs: 5    lr: 0.0001    opt: Adam    init: zero    act: relu  Test MSE: 8.064734240409446e-07\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epochs: 5    lr: 0.001    opt: Adam    init: zero    act: relu\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Train on 801 samples, validate on 801 samples\n",
      "Epoch 1/5\n",
      "801/801 [==============================] - 6s 7ms/step - loss: 2.6612e-06 - val_loss: 8.0647e-07\n",
      "Epoch 2/5\n",
      "801/801 [==============================] - 4s 6ms/step - loss: 2.6612e-06 - val_loss: 8.0647e-07\n",
      "Epoch 3/5\n",
      "801/801 [==============================] - 4s 6ms/step - loss: 2.6612e-06 - val_loss: 8.0647e-07\n",
      "Epoch 4/5\n",
      "801/801 [==============================] - 4s 6ms/step - loss: 2.6612e-06 - val_loss: 8.0647e-07\n",
      "Epoch 5/5\n",
      "801/801 [==============================] - 4s 6ms/step - loss: 2.6612e-06 - val_loss: 8.0647e-07\n",
      "801/801 [==============================] - 1s 1ms/step\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "epochs: 5    lr: 0.001    opt: Adam    init: zero    act: relu  Test MSE: 8.064734240409446e-07\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epochs: 5    lr: 0.01    opt: Adam    init: zero    act: relu\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Train on 801 samples, validate on 801 samples\n",
      "Epoch 1/5\n",
      "801/801 [==============================] - 6s 7ms/step - loss: 2.6612e-06 - val_loss: 8.0647e-07\n",
      "Epoch 2/5\n",
      "801/801 [==============================] - 4s 6ms/step - loss: 2.6612e-06 - val_loss: 8.0647e-07\n",
      "Epoch 3/5\n",
      "801/801 [==============================] - 4s 6ms/step - loss: 2.6612e-06 - val_loss: 8.0647e-07\n",
      "Epoch 4/5\n",
      "801/801 [==============================] - 4s 6ms/step - loss: 2.6612e-06 - val_loss: 8.0647e-07\n",
      "Epoch 5/5\n",
      "801/801 [==============================] - 4s 6ms/step - loss: 2.6612e-06 - val_loss: 8.0647e-07\n",
      "801/801 [==============================] - 1s 1ms/step\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "epochs: 5    lr: 0.01    opt: Adam    init: zero    act: relu  Test MSE: 8.064734240409446e-07\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epochs: 5    lr: 0.1    opt: Adam    init: zero    act: relu\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Train on 801 samples, validate on 801 samples\n",
      "Epoch 1/5\n",
      "801/801 [==============================] - 6s 7ms/step - loss: 2.6612e-06 - val_loss: 8.0647e-07\n",
      "Epoch 2/5\n",
      "801/801 [==============================] - 4s 6ms/step - loss: 2.6612e-06 - val_loss: 8.0647e-07\n",
      "Epoch 3/5\n",
      "801/801 [==============================] - 4s 6ms/step - loss: 2.6612e-06 - val_loss: 8.0647e-07\n",
      "Epoch 4/5\n",
      "801/801 [==============================] - 4s 6ms/step - loss: 2.6612e-06 - val_loss: 8.0647e-07\n",
      "Epoch 5/5\n",
      "801/801 [==============================] - 4s 6ms/step - loss: 2.6612e-06 - val_loss: 8.0647e-07\n",
      "801/801 [==============================] - 1s 1ms/step\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "epochs: 5    lr: 0.1    opt: Adam    init: zero    act: relu  Test MSE: 8.064734240409446e-07\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n"
     ]
    }
   ],
   "source": [
    "predict(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-ea9c21c0dfed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_train_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my_test_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplot_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Training'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplot_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Testing'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "y_train_pred = model.predict(X_train).flatten()\n",
    "y_test_pred = model.predict(X_test).flatten()\n",
    "\n",
    "plot_output(y_train, y_train_pred, 'Training')\n",
    "plot_output(y_test, y_test_pred, 'Testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_y_stats(s, y):\n",
    "    num_spaces = 2\n",
    "    if isinstance(y, pd.core.series.Series):\n",
    "        y_flat = y\n",
    "    elif isinstance(y, np.ndarray):\n",
    "        y_flat = y.flatten()\n",
    "    else:\n",
    "        raise ValueError('Could not process type:', type(y))\n",
    "        \n",
    "    print(s)\n",
    "    print(' ' * num_spaces, 'min: ', min(y_flat))\n",
    "    print(' ' * num_spaces, 'max: ', max(y_flat))\n",
    "    print(' ' * num_spaces, 'mean:', np.mean(y_flat))\n",
    "    print(' ' * num_spaces, 'std: ', np.std(y_flat))\n",
    "    return min(y_flat), max(y_flat), np.mean(y_flat), np.std(y_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, _, y_train_std = print_y_stats('y_train:', y_train)\n",
    "print()\n",
    "print_y_stats('y_test:', y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try filtering values that are 20 standard deviations above the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = np.std(y_train)\n",
    "y_train_filtered = y_train[y_train < 20*std] # Remove all values larger than 20 standard deviations\n",
    "\n",
    "std = np.std(y_test)\n",
    "y_test_filtered = y_test[y_test < 20*std]   # Remove all values larger than 20 standard deviations\n",
    "\n",
    "print('Number of values filtered from y_train:', len(y_train[y_train > 20*std]))\n",
    "print('Number of values filtered from y_test:', len(y_test[y_test > 20*std]))\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "plt.plot(y_train_filtered, color='blue');\n",
    "plt.plot(y_test_filtered, color='red');\n",
    "plt.legend(labels=['training set data', 'testing set data'], prop={'size': 20})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With our threshold of 20 * std, we have removed 13 points from y_train and 12 from y_test. This is out of 1.6 million points, so they were defintely outliers. Let's run the prediction again with the updated y values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_updated = np.where(y_train.flatten() < 20*std, y_train.flatten(), [0.]*len(y_train.flatten()))\n",
    "y_test_updated  = np.where(y_test.flatten() < 20*std, y_test.flatten(), [0.]*len(y_test.flatten()))\n",
    "\n",
    "predict(X_train, \n",
    "        y_train_updated.reshape((801, 12, 168)), \n",
    "        X_test, \n",
    "        y_test_updated.reshape((801, 12, 168)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_updated_pred = model.predict(X_train).flatten()\n",
    "y_test_updated_pred = model.predict(X_test).flatten()\n",
    "\n",
    "plot_output(y_train_updated, y_train_updated_pred, 'Training')\n",
    "plot_output(y_test_updated, y_test_updated_pred, 'Testing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of y-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "pd.Series(y_train_updated).hist(bins=np.arange(0.0001, 0.002, 0.0002));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "pd.Series(y_test_updated).hist(bins=np.arange(0.0001, 0.002, 0.0002));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This data seems reasonable, although it is a little lopsided. Still, it shouldn't cause the neural network to give us the large error that it is giving. Let's try giving the network the full set of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the full set of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetDataFull(object):\n",
    "    def go(self, SESSION, start_year, end_year):\n",
    "        SQL_QUERY = \\\n",
    "            f'''\n",
    "            SELECT incident.blockid, \n",
    "                    incident.year, \n",
    "                    incident.month, \n",
    "                    incident.dow, \n",
    "                    incident.hour,\n",
    "                    SUM(crimetype.severity)/AVG(block.population) AS severity\n",
    "            FROM incident\n",
    "            INNER JOIN block ON incident.blockid = block.id INNER JOIN crimetype ON incident.crimetypeid = crimetype.id AND block.population > 0\n",
    "                AND block.population > 0\n",
    "                AND severity > 0\n",
    "                AND incident.cityid = 1\n",
    "                AND incident.year >= {start_year}\n",
    "                AND incident.year <= {end_year}\n",
    "            GROUP BY\n",
    "                incident.blockid,\n",
    "                incident.year,\n",
    "                incident.month,\n",
    "                incident.dow,\n",
    "                incident.hour\n",
    "            '''\n",
    "        return SESSION.execute(text(SQL_QUERY)).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def days_in_month(year, month):\n",
    "    p = pd.Period(f'{year}-{month}-1')\n",
    "    return p.days_in_month\n",
    "\n",
    "def day_of_week(dt):\n",
    "    return dt.weekday()\n",
    "\n",
    "def create_arrays(blockids, start_year, end_year):\n",
    "    idx = 0\n",
    "    X_blockid, X_year, X_month, X_dow, X_hour, X_risk = [], [], [], [], [], []\n",
    "    for blockid in blockids:\n",
    "        for year in range(start_year, end_year + 1):\n",
    "            for month in range(1, 12 + 1):      # month range is 1-12\n",
    "                for day in range(1, days_in_month(year, month) + 1):\n",
    "                    for hour in range(24):      # hour range is 0-23\n",
    "                        X_blockid.append(blockid)\n",
    "                        X_year.append(year)\n",
    "                        X_month.append(month)\n",
    "                        X_dow.append(day_of_week(datetime(year, month, day)))\n",
    "                        X_hour.append(hour)\n",
    "                        X_risk.append(0.0)\n",
    "                        idx += 1\n",
    "    \n",
    "    X = pd.DataFrame({'blockid':  X_blockid,\n",
    "                      'year':     X_year,\n",
    "                      'month':    X_month,\n",
    "                      'dow':      X_dow,\n",
    "                      'hour':     X_hour,\n",
    "                      'risk':     X_risk})\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_full(data, start_year, end_year):\n",
    "\n",
    "    def remove_outliers_from_risk(risk):\n",
    "        std = np.std(risk)\n",
    "        risk = np.where(risk < 20*std, \n",
    "                     risk, \n",
    "                     [0.]*len(risk)).reshape(risk.shape)\n",
    "\n",
    "        return risk\n",
    "    \n",
    "    NUM_BLOCKIDS = 801\n",
    "    \n",
    "    delta_years = end_year - start_year + 1\n",
    "    \n",
    "    blockid_dict = {}\n",
    "\n",
    "    # Create random array (BLOCKIDS) from 1-801 inclusive\n",
    "    # of length NUM_BLOCKIDS\n",
    "    BLOCKIDS = random.choices(list(range(1,802)), k=NUM_BLOCKIDS)\n",
    "    \n",
    "    for ind, blockid in enumerate(BLOCKIDS):\n",
    "        blockid_dict[blockid] = ind\n",
    "\n",
    "    blockids = list(blockid_dict.values())\n",
    "    X = create_arrays(blockids, start_year, end_year)\n",
    "\n",
    "    # records is the list of rows we get from the query with this order:\n",
    "    #   blockid, year, month, dow, hour, risk\n",
    "    #   month is from 1 - 12\n",
    "\n",
    "    X1 = []\n",
    "    for r in data:\n",
    "        if r[0] in blockid_dict:            \n",
    "            X1.append((r[0], r[1], r[2], r[3], r[4], r[5]))\n",
    "\n",
    "    X1 = pd.DataFrame(data=X1,\n",
    "                      columns=['blockid', 'year', 'month', 'dow', 'hour','risk2'])\n",
    "    X = pd.merge(X, X1, \n",
    "                 how='left',\n",
    "                 left_on=['blockid', 'year', 'month', 'dow', 'hour'],\n",
    "                 right_on=['blockid', 'year', 'month', 'dow', 'hour']\n",
    "                )\n",
    "    X['all_risk'] = X.risk.astype(float) + X.risk2.astype(float)\n",
    "    X = X.drop(columns=['risk', 'risk2']) \\\n",
    "         .rename(mapper={'all_risk': 'risk'}, axis=1)\n",
    "    \n",
    "    y = X['risk'].copy()\n",
    "    X = X.drop(columns=['risk']).copy()\n",
    "    y = remove_outliers_from_risk(y)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ready_data_full(training_start_year, training_end_year,\n",
    "                    testing_start_year, testing_end_year):\n",
    "    with session_scope() as session:\n",
    "        training_data = GetDataFull().go(session,\n",
    "                                         training_start_year,\n",
    "                                         training_end_year)\n",
    "        testing_data = GetDataFull().go(session,\n",
    "                                         testing_start_year,\n",
    "                                         testing_end_year)\n",
    "        X_train, y_train = process_data_full(training_data,\n",
    "                                             training_start_year, \n",
    "                                             training_end_year)\n",
    "        X_test, y_test = process_data_full(testing_data,\n",
    "                                           testing_start_year, \n",
    "                                           testing_end_year)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_train, X_test, y_train, y_test = ready_data_full(2015, 2016, 2017, 2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_full(X_train, y_train, X_test, y_test):\n",
    "#     data_dim    = 5 # 7 * 24   # All values in each hour of the week\n",
    "#     timesteps   = 2 * 12   # Summed per month\n",
    "#     batch_size  = 64\n",
    "#     num_outputs = 7 * 24 * 12\n",
    "\n",
    "#     # expected input data shape: (batch_size, timesteps, data_dim)\n",
    "#     model = Sequential()\n",
    "#     model.add(LSTM(1024, return_sequences=True, \n",
    "# #                    input_shape=(timesteps, # 24\n",
    "# #                                 data_dim), # 168\n",
    "#                    activation='relu',\n",
    "#                    kernel_initializer='random_uniform',\n",
    "#                    bias_initializer='zeros'\n",
    "#                   )\n",
    "#              )\n",
    "#     model.add(LSTM(1024, return_sequences=True, input_shape=(timesteps, data_dim)))\n",
    "#     model.add(LSTM(128, input_shape=(timesteps, data_dim), activation='relu'))\n",
    "#     model.add(Dense(num_outputs, activation='relu'))\n",
    "#     model.add(Reshape((12, 7 * 24)))\n",
    "\n",
    "#     model.compile(loss='mean_squared_error',\n",
    "#                    optimizer=Adam(lr=0.1))\n",
    "\n",
    "#     history = model.fit(X_train, y_train,\n",
    "#                         batch_size=batch_size, epochs=10,\n",
    "#                         validation_data=(X_test, y_test))\n",
    "\n",
    "#     mse = model.evaluate(X_test, y_test,\n",
    "#                          batch_size=batch_size)\n",
    "#     print('Test MSE:', mse)\n",
    "#     return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# history, model = predict_full(X_train, y_train, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
