{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/e0/be401c003291b56efc55aeba6a80ab790d3d4cece2778288d65323009420/pip-19.1.1-py2.py3-none-any.whl (1.4MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4MB 5.1MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Found existing installation: pip 19.1\n",
      "    Uninstalling pip-19.1:\n",
      "      Successfully uninstalled pip-19.1\n",
      "Successfully installed pip-19.1.1\n",
      "Requirement already satisfied: python-decouple in /anaconda3/lib/python3.7/site-packages (3.1)\n",
      "Requirement already satisfied: geoalchemy2 in /anaconda3/lib/python3.7/site-packages (0.6.2)\n",
      "Requirement already satisfied: SQLAlchemy>=0.8 in /anaconda3/lib/python3.7/site-packages (from geoalchemy2) (1.2.15)\n",
      "Requirement already satisfied: shapely in /anaconda3/lib/python3.7/site-packages (1.6.4.post2)\n",
      "Requirement already up-to-date: imbalanced-learn in /anaconda3/lib/python3.7/site-packages (0.4.3)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.13.3 in /anaconda3/lib/python3.7/site-packages (from imbalanced-learn) (1.2.1)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn>=0.20 in /anaconda3/lib/python3.7/site-packages (from imbalanced-learn) (0.20.3)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.8.2 in /anaconda3/lib/python3.7/site-packages (from imbalanced-learn) (1.16.0)\n",
      "Requirement already satisfied: scipy in /anaconda3/lib/python3.7/site-packages (1.2.1)\n",
      "Requirement already satisfied: numpy>=1.8.2 in /anaconda3/lib/python3.7/site-packages (from scipy) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install python-decouple\n",
    "!pip install geoalchemy2\n",
    "!pip install shapely\n",
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine, func, text\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from decouple import config\n",
    "from shapely import wkb, wkt\n",
    "from shapely.geometry import Point\n",
    "from geoalchemy2.shape import to_shape \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# ----------- TODO: Issues with importing imbalance-learn library\n",
    "# from imblearn.over_sampling import RandomOverSampler\n",
    "# from imblearn.under_sampling import NearMiss\n",
    "# from imblearn.under_sampling import (RandomUnderSampler,\n",
    "#                                      ClusterCentroids,\n",
    "#                                      TomekLinks,\n",
    "#                                      NeighbourhoodCleaningRule,\n",
    "#                                      NearMiss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Contains models for DB.\"\"\"\n",
    "\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy import Column, BigInteger, Integer, String, DateTime, ForeignKey, Float\n",
    "from sqlalchemy.orm import relationship\n",
    "from geoalchemy2 import Geometry\n",
    "\n",
    "\n",
    "BASE = declarative_base()\n",
    "\n",
    "\n",
    "class City(BASE):\n",
    "    \"\"\"City model for DB. Has information of cities.\"\"\"\n",
    "    __tablename__ = 'city'\n",
    "    id            = Column(BigInteger, primary_key=True)\n",
    "    city          = Column(String, unique=False, nullable=False)\n",
    "    state         = Column(String, unique=False, nullable=True)\n",
    "    country       = Column(String, unique=False, nullable=False)\n",
    "    location      = Column(Geometry(geometry_type='POINT'), nullable=False)\n",
    "    blocks        = relationship(\"Blocks\", back_populates=\"city\")\n",
    "    zipcodes      = relationship(\"ZipcodeGeom\", back_populates=\"city\")\n",
    "    incidents     = relationship(\"Incident\", back_populates=\"city\")\n",
    "\n",
    "\n",
    "class Blocks(BASE):\n",
    "    \"\"\"Block model for DB. Has information of city blocks for a related city\n",
    "        id.\"\"\"\n",
    "    __tablename__ = 'block'\n",
    "    id            = Column(BigInteger, primary_key=True)\n",
    "    cityid        = Column(BigInteger, ForeignKey('city.id'), nullable=False)\n",
    "    shape         = Column(Geometry(geometry_type='MULTIPOLYGON'), nullable=False)\n",
    "    population    = Column(Integer, nullable=False)\n",
    "    city          = relationship(\"City\", back_populates=\"blocks\")\n",
    "    incidents     = relationship(\"Incident\", back_populates=\"block\")\n",
    "\n",
    "class ZipcodeGeom(BASE):\n",
    "    \"\"\"Zipcode geometry model for DB. Has information of zipcodes and related\n",
    "        city id.\"\"\"\n",
    "    __tablename__ = 'zipcodegeom'\n",
    "    id            = Column(BigInteger, primary_key=True)\n",
    "    cityid        = Column(BigInteger, ForeignKey('city.id'), nullable=False)\n",
    "    zipcode       = Column(String, nullable=False, unique=True)\n",
    "    shape         = Column(Geometry(geometry_type='MULTIPOLYGON'), nullable=False)\n",
    "    city          = relationship(\"City\", back_populates=\"zipcodes\")\n",
    "\n",
    "class Incident(BASE):\n",
    "    \"\"\"Incident model for DB. Has information of a specific crime, including\n",
    "        where it took place, when it took place, and the type of crime that\n",
    "        occurred.\"\"\"\n",
    "    __tablename__ = 'incident'\n",
    "    id            = Column(BigInteger, primary_key=True)\n",
    "    crimetypeid   = Column(BigInteger, ForeignKey('crimetype.id'), nullable=False)\n",
    "    locdescid     = Column(BigInteger, ForeignKey('locdesctype.id'), nullable=False)\n",
    "    cityid        = Column(BigInteger, ForeignKey('city.id'), nullable=False)\n",
    "    blockid       = Column(BigInteger, ForeignKey('block.id'), nullable=False)\n",
    "    location      = Column(Geometry(geometry_type='POINT'), nullable=False)\n",
    "    datetime      = Column(DateTime, nullable=False)\n",
    "    hour          = Column(Integer, nullable=False)\n",
    "    dow           = Column(Integer, nullable=False)\n",
    "    month         = Column(Integer, nullable=False)\n",
    "    year          = Column(Integer, nullable=False)\n",
    "    city          = relationship(\"City\", back_populates=\"incidents\")\n",
    "    block         = relationship(\"Blocks\", back_populates=\"incidents\")\n",
    "    crimetype     = relationship(\"CrimeType\", back_populates=\"incidents\")\n",
    "    locationdesc  = relationship(\"LocationDescriptionType\", back_populates=\"incidents\")\n",
    "\n",
    "class CrimeType(BASE):\n",
    "    \"\"\"CrimeType model for DB. Has information of the types of crime, including\n",
    "        a general description and the numerical severity of the crime.\"\"\"\n",
    "    __tablename__ = 'crimetype'\n",
    "    id            = Column(BigInteger, primary_key=True)\n",
    "    category      = Column(String, unique=True, nullable=False)\n",
    "    severity      = Column(Integer, nullable=False)\n",
    "    incidents     = relationship(\"Incident\", back_populates=\"crimetype\")\n",
    "\n",
    "\n",
    "class LocationDescriptionType(BASE):\n",
    "    \"\"\"Location description model for DB. Has information on the type of\n",
    "        location that the crime took place.\"\"\"\n",
    "    __tablename__ = 'locdesctype'\n",
    "    id            = Column(BigInteger, primary_key=True)\n",
    "    key1          = Column(String, nullable=False)\n",
    "    key2          = Column(String, nullable=False)\n",
    "    key3          = Column(String, nullable=False)\n",
    "    incidents     = relationship(\"Incident\", back_populates=\"locationdesc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetData(object):\n",
    "    def go(self, SESSION, start_year, end_year):\n",
    "        SQL_QUERY = \\\n",
    "            f'''\n",
    "                WITH\n",
    "                    max_severity AS (\n",
    "                        SELECT MAX(severity) AS severity\n",
    "                        FROM (\n",
    "                            SELECT SUM(crimetype.severity)/AVG(block.population) AS severity\n",
    "                            FROM incident\n",
    "                            INNER JOIN block ON incident.blockid = block.id INNER JOIN crimetype ON incident.crimetypeid = crimetype.id AND block.population > 0\n",
    "                            GROUP BY\n",
    "                                incident.blockid,\n",
    "                                incident.year,\n",
    "                                incident.month,\n",
    "                                incident.dow,\n",
    "                                incident.hour\n",
    "                        ) AS categories\n",
    "                    ),\n",
    "                    block_incidents AS (\n",
    "                        SELECT\n",
    "                            incident.blockid,\n",
    "                            incident.year,\n",
    "                            incident.month,\n",
    "                            incident.dow,\n",
    "                            incident.hour,\n",
    "                            SUM(crimetype.severity)/AVG(block.population) AS severity\n",
    "                        FROM incident\n",
    "                        INNER JOIN block ON incident.blockid = block.id\n",
    "                        INNER JOIN crimetype ON incident.crimetypeid = crimetype.id\n",
    "                            AND block.population > 0\n",
    "                            AND incident.cityid = 1\n",
    "                            AND incident.year >= {start_year}\n",
    "                            AND incident.year <= {end_year}\n",
    "                        GROUP BY\n",
    "                            incident.blockid,\n",
    "                            incident.year,\n",
    "                            incident.month,\n",
    "                            incident.dow,\n",
    "                            incident.hour\n",
    "                    )\n",
    "                SELECT\n",
    "                    block_incidents.blockid,\n",
    "                    block_incidents.year,\n",
    "                    block_incidents.month,\n",
    "                    block_incidents.dow,\n",
    "                    block_incidents.hour,\n",
    "                    block_incidents.severity/max_severity.severity AS severity\n",
    "                FROM block_incidents, max_severity        \n",
    "            '''\n",
    "        return SESSION.execute(text(SQL_QUERY)).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data, start_year, end_year, map_risk):\n",
    "\n",
    "    NUM_BLOCKIDS = 801\n",
    "    \n",
    "    X = np.zeros((NUM_BLOCKIDS, 24, 7*24))\n",
    "    y = np.zeros((NUM_BLOCKIDS, 12, 7*24))\n",
    "    blockid_dict = {}\n",
    "\n",
    "    # Create random array (BLOCKIDS) from 1-801 inclusive\n",
    "    # of length NUM_BLOCKIDS\n",
    "    BLOCKIDS = random.choices(list(range(1,802)), k=NUM_BLOCKIDS)\n",
    "    \n",
    "    for ind, blockid in enumerate(BLOCKIDS):\n",
    "        blockid_dict[blockid] = ind\n",
    "\n",
    "    # records is the list of rows we get from the query with this order:\n",
    "    #   blockid, year, month, dow, hour, risk\n",
    "    #   month is from 1 - 12\n",
    "\n",
    "    for r in data:\n",
    "        if r[0] in blockid_dict:\n",
    "            if r[1] == end_year:\n",
    "                # index into array  0-based month\n",
    "                # vvvvvvvvvvvvvvvv    vvvvvv\n",
    "                y[blockid_dict[r[0]], r[2]-1, 24*r[3]+r[4]] = map_risk(float(r[5]))\n",
    "                #                             ^^^^^^^^^^^^^   ^^^^\n",
    "                #                             hours since     risk\n",
    "                #                             beginning of\n",
    "                #                             week\n",
    "            else:\n",
    "                # index into array    year 0.....1   month   \n",
    "                # vvvvvvvvvvvvvvvv    vvvvvvvvvvvvv  vvvvvv\n",
    "                X[blockid_dict[r[0]], 12*(r[1]-start_year)+r[2]-1, 24*r[3]+r[4]] = map_risk(float(r[5]))\n",
    "                #                                                  ^^^^^^^^^^^^^   ^^^^\n",
    "                #                                                  hours since     risk\n",
    "                #                                                  beginning of\n",
    "                #                                                  week                \n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def session_scope():\n",
    "    \"\"\"Provide a transactional scope around a series of operations.\"\"\"\n",
    "\n",
    "    DB_URI  = config('DB_URI')\n",
    "    ENGINE  = create_engine(DB_URI)\n",
    "    Session = sessionmaker(bind=ENGINE)\n",
    "    SESSION = Session()\n",
    "    \n",
    "    try:\n",
    "        yield SESSION\n",
    "        SESSION.commit()\n",
    "    except:\n",
    "        SESSION.rollback()\n",
    "        raise\n",
    "    finally:\n",
    "        SESSION.close()\n",
    "\n",
    "\n",
    "def ready_data(training_start_year, training_end_year,\n",
    "               testing_start_year, testing_end_year,\n",
    "               map_risk):\n",
    "    with session_scope() as session:\n",
    "        training_data = GetData().go(session,\n",
    "                                     training_start_year,\n",
    "                                     training_end_year)\n",
    "        testing_data = GetData().go(session,\n",
    "                                     testing_start_year,\n",
    "                                     testing_end_year)\n",
    "        \n",
    "        X_train, y_train = process_data(training_data,\n",
    "                                        training_start_year, \n",
    "                                        training_end_year,\n",
    "                                        map_risk)\n",
    "        X_test, y_test = process_data(testing_data,\n",
    "                                      testing_start_year, \n",
    "                                      testing_end_year,\n",
    "                                      map_risk)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "def map_risk_1(risk):\n",
    "    return np.power(risk * 1e5, 2)\n",
    "\n",
    "def map_risk_0(risk):\n",
    "    return risk\n",
    "\n",
    "def map_risk_2(risk):\n",
    "    return np.power(risk * 1e5, 3)\n",
    "\n",
    "def map_risk_3(risk):\n",
    "    return np.power(risk * 1e5, 4)\n",
    "\n",
    "map_risk = [map_risk_1, map_risk_2, map_risk_3]  # , map_risk_1, map_risk_2, map_risk_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(801, 24, 168) (801, 12, 168) (801, 24, 168) (801, 12, 168)\n",
      "CPU times: user 1.54 s, sys: 269 ms, total: 1.81 s\n",
      "Wall time: 53.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train, X_test, y_train, y_test = ready_data(2015, 2016, 2017, 2017, map_risk_0)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((801, 24, 168), (801, 12, 168), (801, 24, 168), (801, 12, 168))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape\n",
    "((801, 24, 168), (801, 12, 168), (801, 24, 168), (801, 12, 168))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_output(y, y_pred, dataset_type):\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    plt.plot(np.arange(len(y.flatten())),\n",
    "                 y.flatten(), color='blue');\n",
    "    plt.plot(np.arange(len(y_pred.flatten())),\n",
    "                 y_pred.flatten(), color='red');\n",
    "    plt.xlabel('Hour since beginning of data', fontsize=16)\n",
    "    plt.ylabel('Risk', fontsize=18)\n",
    "    plt.title(dataset_type + ' dataset', fontsize=18)\n",
    "    plt.legend(labels=['risk', 'predicted risk'], prop={'size': 20})\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_validation_loss(epochs, history):\n",
    "    plt.plot(range(1, epochs), history.history['loss'])\n",
    "    plt.plot(range(1, epochs), history.history['val_loss'])\n",
    "    plt.title('model train vs validation loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Reshape, Dropout\n",
    "from keras.optimizers import Adam, SGD, RMSprop, Adagrad, Adadelta, Adamax, Nadam\n",
    "import keras.backend as K\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "def perc_error(y_true, y_pred):\n",
    "    return 100.0 * K.mean((y_true - y_pred) / y_true)\n",
    "\n",
    "def create_model(learn_rate=0.01, \n",
    "                 momentum=0,\n",
    "                 opt_name='Adam',\n",
    "                 init_mode='uniform',\n",
    "                 activation='relu'\n",
    "                ):\n",
    "    data_dim    = 7 * 24   # All values in each hour of the week\n",
    "                           # averaged over each day for all weeks\n",
    "                           # of the month\n",
    "    timesteps   = 2 * 12   # Summed per month\n",
    "    batch_size  = 64\n",
    "    num_outputs = 7 * 24 * 12\n",
    "    \n",
    "    # expected input data shape: (batch_size, timesteps, data_dim)\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(1024, input_shape=(24,168), activation='relu'))\n",
    "    model.add(Dense(12*168, kernel_initializer='uniform', activation='relu'))\n",
    "    model.add(Reshape((12,168)))\n",
    "    \n",
    "    optimizer = { \n",
    "        'SGD':      SGD(lr=learn_rate, momentum=momentum),\n",
    "        'RMSprop':  RMSprop(lr=learn_rate),\n",
    "        'Adagrad':  Adagrad(lr=learn_rate),\n",
    "        'Adadelta': Adadelta(lr=learn_rate),\n",
    "        'Adam':     Adam(lr=learn_rate),\n",
    "        'Adamax':   Adamax(lr=learn_rate),\n",
    "        'Nadam':    Nadam(lr=learn_rate),\n",
    "    }\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer[opt_name])\n",
    "\n",
    "    return model\n",
    "\n",
    "def predict(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    # GridSearchCV's fit method requires X which is 2D, y which is 1D.\n",
    "    # This is a problem for us since our X and y are 3D.\n",
    "    # Instead of GridSearchCV, we will create our own loop to\n",
    "    # search through the grid.\n",
    "    # \n",
    "    batch_size = 64\n",
    "    for epochs in range(5, 6, 5):\n",
    "        for lr in [0.00001, 0.0001, 0.001, 0.01, 0.1]:\n",
    "            for opt_name in ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']:\n",
    "                for init_mode in ['uniform', 'lecun_uniform', 'normal', 'zero',\n",
    "                                  'glorot_normal', 'glorot_uniform', 'he_normal',\n",
    "                                  'he_uniform']:\n",
    "                    for activation in ['relu']:\n",
    "\n",
    "                        print('>'*80)\n",
    "                        print('epochs:', epochs, '   lr:', lr, \\\n",
    "                              '   opt:', opt_name, \\\n",
    "                              '   init:', init_mode, \\\n",
    "                              '   act:', 'relu')\n",
    "                        print('>'*80)\n",
    "\n",
    "                        model = create_model()\n",
    "\n",
    "                        model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "                        history = model.fit(X_train, y_train,\n",
    "                                            batch_size=batch_size, \n",
    "                                            epochs=epochs,\n",
    "                                            validation_data=(X_test, y_test))\n",
    "\n",
    "                        mse = model.evaluate(X_test, y_test,\n",
    "                                             batch_size=batch_size)\n",
    "                        print('<'*80)\n",
    "                        print('epochs:', epochs, '   lr:', lr, \\\n",
    "                              '   opt:', opt_name, \\\n",
    "                              '   init:', init_mode, \\\n",
    "                              '   act:', 'relu', \\\n",
    "                              ' Test MSE:', mse \\\n",
    "                        )\n",
    "                        print('<'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epochs: 5    lr: 1e-05    opt: SGD    init: uniform    act: relu\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 801 samples, validate on 801 samples\n",
      "Epoch 1/5\n",
      "801/801 [==============================] - 11s 13ms/step - loss: 2.7313e-07 - val_loss: 1.0190e-07\n",
      "Epoch 2/5\n",
      "801/801 [==============================] - 2s 2ms/step - loss: 2.7214e-07 - val_loss: 1.0180e-07\n",
      "Epoch 3/5\n",
      "801/801 [==============================] - 2s 2ms/step - loss: 2.7214e-07 - val_loss: 1.0184e-07\n",
      "Epoch 4/5\n",
      "801/801 [==============================] - 2s 2ms/step - loss: 2.7218e-07 - val_loss: 1.0187e-07\n",
      "Epoch 5/5\n",
      "801/801 [==============================] - 2s 2ms/step - loss: 2.7221e-07 - val_loss: 1.0190e-07\n",
      "801/801 [==============================] - 0s 496us/step\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "epochs: 5    lr: 1e-05    opt: SGD    init: uniform    act: relu  Test MSE: 1.0190068563790464e-07\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epochs: 5    lr: 1e-05    opt: SGD    init: lecun_uniform    act: relu\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Train on 801 samples, validate on 801 samples\n",
      "Epoch 1/5\n",
      "801/801 [==============================] - 3s 4ms/step - loss: 2.7266e-07 - val_loss: 1.0305e-07\n",
      "Epoch 2/5\n",
      "801/801 [==============================] - 2s 2ms/step - loss: 2.7232e-07 - val_loss: 1.0173e-07\n",
      "Epoch 3/5\n",
      "801/801 [==============================] - 2s 2ms/step - loss: 2.7207e-07 - val_loss: 1.0178e-07\n",
      "Epoch 4/5\n",
      "801/801 [==============================] - 2s 2ms/step - loss: 2.7214e-07 - val_loss: 1.0184e-07\n",
      "Epoch 5/5\n",
      "801/801 [==============================] - 2s 2ms/step - loss: 2.7217e-07 - val_loss: 1.0186e-07\n",
      "801/801 [==============================] - 0s 482us/step\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "epochs: 5    lr: 1e-05    opt: SGD    init: lecun_uniform    act: relu  Test MSE: 1.0186427900489049e-07\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epochs: 5    lr: 1e-05    opt: SGD    init: normal    act: relu\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Train on 801 samples, validate on 801 samples\n",
      "Epoch 1/5\n",
      "128/801 [===>..........................] - ETA: 8s - loss: 8.8587e-09 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-a1c73b392e00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-9fbf84503c1e>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[1;32m     68\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                                             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                                             validation_data=(X_test, y_test))\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                         mse = model.evaluate(X_test, y_test,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "predict(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = model.predict(X_train).flatten()\n",
    "y_test_pred = model.predict(X_test).flatten()\n",
    "\n",
    "plot_output(y_train, y_train_pred, 'Training')\n",
    "plot_output(y_test, y_test_pred, 'Testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_y_stats(s, y):\n",
    "    num_spaces = 2\n",
    "    if isinstance(y, pd.core.series.Series):\n",
    "        y_flat = y\n",
    "    elif isinstance(y, np.ndarray):\n",
    "        y_flat = y.flatten()\n",
    "    else:\n",
    "        raise ValueError('Could not process type:', type(y))\n",
    "        \n",
    "    print(s)\n",
    "    print(' ' * num_spaces, 'min: ', min(y_flat))\n",
    "    print(' ' * num_spaces, 'max: ', max(y_flat))\n",
    "    print(' ' * num_spaces, 'mean:', np.mean(y_flat))\n",
    "    print(' ' * num_spaces, 'std: ', np.std(y_flat))\n",
    "    return min(y_flat), max(y_flat), np.mean(y_flat), np.std(y_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, _, y_train_std = print_y_stats('y_train:', y_train)\n",
    "print()\n",
    "print_y_stats('y_test:', y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try filtering values that are 20 standard deviations above the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = np.std(y_train)\n",
    "y_train_filtered = y_train[y_train < 20*std] # Remove all values larger than 20 standard deviations\n",
    "\n",
    "std = np.std(y_test)\n",
    "y_test_filtered = y_test[y_test < 20*std]   # Remove all values larger than 20 standard deviations\n",
    "\n",
    "print('Number of values filtered from y_train:', len(y_train[y_train > 20*std]))\n",
    "print('Number of values filtered from y_test:', len(y_test[y_test > 20*std]))\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "plt.plot(y_train_filtered, color='blue');\n",
    "plt.plot(y_test_filtered, color='red');\n",
    "plt.legend(labels=['training set data', 'testing set data'], prop={'size': 20})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With our threshold of 20 * std, we have removed 13 points from y_train and 12 from y_test. This is out of 1.6 million points, so they were defintely outliers. Let's run the prediction again with the updated y values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_updated = np.where(y_train.flatten() < 20*std, y_train.flatten(), [0.]*len(y_train.flatten()))\n",
    "y_test_updated  = np.where(y_test.flatten() < 20*std, y_test.flatten(), [0.]*len(y_test.flatten()))\n",
    "\n",
    "predict(X_train, \n",
    "        y_train_updated.reshape((801, 12, 168)), \n",
    "        X_test, \n",
    "        y_test_updated.reshape((801, 12, 168)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_updated_pred = model.predict(X_train).flatten()\n",
    "y_test_updated_pred = model.predict(X_test).flatten()\n",
    "\n",
    "plot_output(y_train_updated, y_train_updated_pred, 'Training')\n",
    "plot_output(y_test_updated, y_test_updated_pred, 'Testing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of y-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "pd.Series(y_train_updated).hist(bins=np.arange(0.0001, 0.002, 0.0002));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "pd.Series(y_test_updated).hist(bins=np.arange(0.0001, 0.002, 0.0002));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This data seems reasonable, although it is a little lopsided. Still, it shouldn't cause the neural network to give us the large error that it is giving. Let's try giving the network the full set of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the full set of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetDataFull(object):\n",
    "    def go(self, SESSION, start_year, end_year):\n",
    "        SQL_QUERY = \\\n",
    "            f'''\n",
    "            SELECT incident.blockid, \n",
    "                    incident.year, \n",
    "                    incident.month, \n",
    "                    incident.dow, \n",
    "                    incident.hour,\n",
    "                    SUM(crimetype.severity)/AVG(block.population) AS severity\n",
    "            FROM incident\n",
    "            INNER JOIN block ON incident.blockid = block.id INNER JOIN crimetype ON incident.crimetypeid = crimetype.id AND block.population > 0\n",
    "                AND block.population > 0\n",
    "                AND severity > 0\n",
    "                AND incident.cityid = 1\n",
    "                AND incident.year >= {start_year}\n",
    "                AND incident.year <= {end_year}\n",
    "            GROUP BY\n",
    "                incident.blockid,\n",
    "                incident.year,\n",
    "                incident.month,\n",
    "                incident.dow,\n",
    "                incident.hour\n",
    "            '''\n",
    "        return SESSION.execute(text(SQL_QUERY)).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def days_in_month(year, month):\n",
    "    p = pd.Period(f'{year}-{month}-1')\n",
    "    return p.days_in_month\n",
    "\n",
    "def day_of_week(dt):\n",
    "    return dt.weekday()\n",
    "\n",
    "def create_arrays(blockids, start_year, end_year):\n",
    "    idx = 0\n",
    "    X_blockid, X_year, X_month, X_dow, X_hour, X_risk = [], [], [], [], [], []\n",
    "    for blockid in blockids:\n",
    "        for year in range(start_year, end_year + 1):\n",
    "            for month in range(1, 12 + 1):      # month range is 1-12\n",
    "                for day in range(1, days_in_month(year, month) + 1):\n",
    "                    for hour in range(24):      # hour range is 0-23\n",
    "                        X_blockid.append(blockid)\n",
    "                        X_year.append(year)\n",
    "                        X_month.append(month)\n",
    "                        X_dow.append(day_of_week(datetime(year, month, day)))\n",
    "                        X_hour.append(hour)\n",
    "                        X_risk.append(0.0)\n",
    "                        idx += 1\n",
    "    \n",
    "    X = pd.DataFrame({'blockid':  X_blockid,\n",
    "                      'year':     X_year,\n",
    "                      'month':    X_month,\n",
    "                      'dow':      X_dow,\n",
    "                      'hour':     X_hour,\n",
    "                      'risk':     X_risk})\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_full(data, start_year, end_year):\n",
    "\n",
    "    def remove_outliers_from_risk(risk):\n",
    "        std = np.std(risk)\n",
    "        risk = np.where(risk < 20*std, \n",
    "                     risk, \n",
    "                     [0.]*len(risk)).reshape(risk.shape)\n",
    "\n",
    "        return risk\n",
    "    \n",
    "    NUM_BLOCKIDS = 801\n",
    "    \n",
    "    delta_years = end_year - start_year + 1\n",
    "    \n",
    "    blockid_dict = {}\n",
    "\n",
    "    # Create random array (BLOCKIDS) from 1-801 inclusive\n",
    "    # of length NUM_BLOCKIDS\n",
    "    BLOCKIDS = random.choices(list(range(1,802)), k=NUM_BLOCKIDS)\n",
    "    \n",
    "    for ind, blockid in enumerate(BLOCKIDS):\n",
    "        blockid_dict[blockid] = ind\n",
    "\n",
    "    blockids = list(blockid_dict.values())\n",
    "    X = create_arrays(blockids, start_year, end_year)\n",
    "\n",
    "    # records is the list of rows we get from the query with this order:\n",
    "    #   blockid, year, month, dow, hour, risk\n",
    "    #   month is from 1 - 12\n",
    "\n",
    "    X1 = []\n",
    "    for r in data:\n",
    "        if r[0] in blockid_dict:            \n",
    "            X1.append((r[0], r[1], r[2], r[3], r[4], r[5]))\n",
    "\n",
    "    X1 = pd.DataFrame(data=X1,\n",
    "                      columns=['blockid', 'year', 'month', 'dow', 'hour','risk2'])\n",
    "    X = pd.merge(X, X1, \n",
    "                 how='left',\n",
    "                 left_on=['blockid', 'year', 'month', 'dow', 'hour'],\n",
    "                 right_on=['blockid', 'year', 'month', 'dow', 'hour']\n",
    "                )\n",
    "    X['all_risk'] = X.risk.astype(float) + X.risk2.astype(float)\n",
    "    X = X.drop(columns=['risk', 'risk2']) \\\n",
    "         .rename(mapper={'all_risk': 'risk'}, axis=1)\n",
    "    \n",
    "    y = X['risk'].copy()\n",
    "    X = X.drop(columns=['risk']).copy()\n",
    "    y = remove_outliers_from_risk(y)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ready_data_full(training_start_year, training_end_year,\n",
    "                    testing_start_year, testing_end_year):\n",
    "    with session_scope() as session:\n",
    "        training_data = GetDataFull().go(session,\n",
    "                                         training_start_year,\n",
    "                                         training_end_year)\n",
    "        testing_data = GetDataFull().go(session,\n",
    "                                         testing_start_year,\n",
    "                                         testing_end_year)\n",
    "        X_train, y_train = process_data_full(training_data,\n",
    "                                             training_start_year, \n",
    "                                             training_end_year)\n",
    "        X_test, y_test = process_data_full(testing_data,\n",
    "                                           testing_start_year, \n",
    "                                           testing_end_year)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_train, X_test, y_train, y_test = ready_data_full(2015, 2016, 2017, 2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_full(X_train, y_train, X_test, y_test):\n",
    "#     data_dim    = 5 # 7 * 24   # All values in each hour of the week\n",
    "#     timesteps   = 2 * 12   # Summed per month\n",
    "#     batch_size  = 64\n",
    "#     num_outputs = 7 * 24 * 12\n",
    "\n",
    "#     # expected input data shape: (batch_size, timesteps, data_dim)\n",
    "#     model = Sequential()\n",
    "#     model.add(LSTM(1024, return_sequences=True, \n",
    "# #                    input_shape=(timesteps, # 24\n",
    "# #                                 data_dim), # 168\n",
    "#                    activation='relu',\n",
    "#                    kernel_initializer='random_uniform',\n",
    "#                    bias_initializer='zeros'\n",
    "#                   )\n",
    "#              )\n",
    "#     model.add(LSTM(1024, return_sequences=True, input_shape=(timesteps, data_dim)))\n",
    "#     model.add(LSTM(128, input_shape=(timesteps, data_dim), activation='relu'))\n",
    "#     model.add(Dense(num_outputs, activation='relu'))\n",
    "#     model.add(Reshape((12, 7 * 24)))\n",
    "\n",
    "#     model.compile(loss='mean_squared_error',\n",
    "#                    optimizer=Adam(lr=0.1))\n",
    "\n",
    "#     history = model.fit(X_train, y_train,\n",
    "#                         batch_size=batch_size, epochs=10,\n",
    "#                         validation_data=(X_test, y_test))\n",
    "\n",
    "#     mse = model.evaluate(X_test, y_test,\n",
    "#                          batch_size=batch_size)\n",
    "#     print('Test MSE:', mse)\n",
    "#     return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# history, model = predict_full(X_train, y_train, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
