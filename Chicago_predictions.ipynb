{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/e0/be401c003291b56efc55aeba6a80ab790d3d4cece2778288d65323009420/pip-19.1.1-py2.py3-none-any.whl (1.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.4MB 6.4MB/s ta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Found existing installation: pip 10.0.1\n",
      "    Uninstalling pip-10.0.1:\n",
      "      Successfully uninstalled pip-10.0.1\n",
      "Successfully installed pip-19.1.1\n",
      "Collecting python-decouple\n",
      "  Downloading https://files.pythonhosted.org/packages/9b/99/ddfbb6362af4ee239a012716b1371aa6d316ff1b9db705bfb182fbc4780f/python-decouple-3.1.tar.gz\n",
      "Building wheels for collected packages: python-decouple\n",
      "  Building wheel for python-decouple (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ec2-user/.cache/pip/wheels/0f/ee/80/75b684060dc6ecc5a28c07b75ef4063f378aff1a37556f342a\n",
      "Successfully built python-decouple\n",
      "Installing collected packages: python-decouple\n",
      "Successfully installed python-decouple-3.1\n",
      "Collecting geoalchemy2\n",
      "  Downloading https://files.pythonhosted.org/packages/f9/39/f50f736246b223d8e22474131eb9e3c3f2eb76d409ed1ef7f8dcde60bdc8/GeoAlchemy2-0.6.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: SQLAlchemy>=0.8 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from geoalchemy2) (1.2.11)\n",
      "Installing collected packages: geoalchemy2\n",
      "Successfully installed geoalchemy2-0.6.2\n",
      "Collecting shapely\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/38/b6/b53f19062afd49bb5abd049aeed36f13bf8d57ef8f3fa07a5203531a0252/Shapely-1.6.4.post2-cp36-cp36m-manylinux1_x86_64.whl (1.5MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5MB 1.5MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: shapely\n",
      "Successfully installed shapely-1.6.4.post2\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install python-decouple\n",
    "!pip install geoalchemy2\n",
    "!pip install shapely\n",
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine, func, text\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from decouple import config\n",
    "from shapely import wkb, wkt\n",
    "from shapely.geometry import Point\n",
    "from geoalchemy2.shape import to_shape \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# ----------- TODO: Issues with importing imbalance-learn library\n",
    "# from imblearn.over_sampling import RandomOverSampler\n",
    "# from imblearn.under_sampling import NearMiss\n",
    "# from imblearn.under_sampling import (RandomUnderSampler,\n",
    "#                                      ClusterCentroids,\n",
    "#                                      TomekLinks,\n",
    "#                                      NeighbourhoodCleaningRule,\n",
    "#                                      NearMiss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Contains models for DB.\"\"\"\n",
    "\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy import Column, BigInteger, Integer, String, DateTime, ForeignKey, Float\n",
    "from sqlalchemy.orm import relationship\n",
    "from geoalchemy2 import Geometry\n",
    "\n",
    "\n",
    "BASE = declarative_base()\n",
    "\n",
    "\n",
    "class City(BASE):\n",
    "    \"\"\"City model for DB. Has information of cities.\"\"\"\n",
    "    __tablename__ = 'city'\n",
    "    id            = Column(BigInteger, primary_key=True)\n",
    "    city          = Column(String, unique=False, nullable=False)\n",
    "    state         = Column(String, unique=False, nullable=True)\n",
    "    country       = Column(String, unique=False, nullable=False)\n",
    "    location      = Column(Geometry(geometry_type='POINT'), nullable=False)\n",
    "    blocks        = relationship(\"Blocks\", back_populates=\"city\")\n",
    "    zipcodes      = relationship(\"ZipcodeGeom\", back_populates=\"city\")\n",
    "    incidents     = relationship(\"Incident\", back_populates=\"city\")\n",
    "\n",
    "\n",
    "class Blocks(BASE):\n",
    "    \"\"\"Block model for DB. Has information of city blocks for a related city\n",
    "        id.\"\"\"\n",
    "    __tablename__ = 'block'\n",
    "    id            = Column(BigInteger, primary_key=True)\n",
    "    cityid        = Column(BigInteger, ForeignKey('city.id'), nullable=False)\n",
    "    shape         = Column(Geometry(geometry_type='MULTIPOLYGON'), nullable=False)\n",
    "    population    = Column(Integer, nullable=False)\n",
    "    city          = relationship(\"City\", back_populates=\"blocks\")\n",
    "    incidents     = relationship(\"Incident\", back_populates=\"block\")\n",
    "\n",
    "class ZipcodeGeom(BASE):\n",
    "    \"\"\"Zipcode geometry model for DB. Has information of zipcodes and related\n",
    "        city id.\"\"\"\n",
    "    __tablename__ = 'zipcodegeom'\n",
    "    id            = Column(BigInteger, primary_key=True)\n",
    "    cityid        = Column(BigInteger, ForeignKey('city.id'), nullable=False)\n",
    "    zipcode       = Column(String, nullable=False, unique=True)\n",
    "    shape         = Column(Geometry(geometry_type='MULTIPOLYGON'), nullable=False)\n",
    "    city          = relationship(\"City\", back_populates=\"zipcodes\")\n",
    "\n",
    "class Incident(BASE):\n",
    "    \"\"\"Incident model for DB. Has information of a specific crime, including\n",
    "        where it took place, when it took place, and the type of crime that\n",
    "        occurred.\"\"\"\n",
    "    __tablename__ = 'incident'\n",
    "    id            = Column(BigInteger, primary_key=True)\n",
    "    crimetypeid   = Column(BigInteger, ForeignKey('crimetype.id'), nullable=False)\n",
    "    locdescid     = Column(BigInteger, ForeignKey('locdesctype.id'), nullable=False)\n",
    "    cityid        = Column(BigInteger, ForeignKey('city.id'), nullable=False)\n",
    "    blockid       = Column(BigInteger, ForeignKey('block.id'), nullable=False)\n",
    "    location      = Column(Geometry(geometry_type='POINT'), nullable=False)\n",
    "    datetime      = Column(DateTime, nullable=False)\n",
    "    hour          = Column(Integer, nullable=False)\n",
    "    dow           = Column(Integer, nullable=False)\n",
    "    month         = Column(Integer, nullable=False)\n",
    "    year          = Column(Integer, nullable=False)\n",
    "    city          = relationship(\"City\", back_populates=\"incidents\")\n",
    "    block         = relationship(\"Blocks\", back_populates=\"incidents\")\n",
    "    crimetype     = relationship(\"CrimeType\", back_populates=\"incidents\")\n",
    "    locationdesc  = relationship(\"LocationDescriptionType\", back_populates=\"incidents\")\n",
    "\n",
    "class CrimeType(BASE):\n",
    "    \"\"\"CrimeType model for DB. Has information of the types of crime, including\n",
    "        a general description and the numerical severity of the crime.\"\"\"\n",
    "    __tablename__ = 'crimetype'\n",
    "    id            = Column(BigInteger, primary_key=True)\n",
    "    category      = Column(String, unique=True, nullable=False)\n",
    "    severity      = Column(Integer, nullable=False)\n",
    "    incidents     = relationship(\"Incident\", back_populates=\"crimetype\")\n",
    "\n",
    "\n",
    "class LocationDescriptionType(BASE):\n",
    "    \"\"\"Location description model for DB. Has information on the type of\n",
    "        location that the crime took place.\"\"\"\n",
    "    __tablename__ = 'locdesctype'\n",
    "    id            = Column(BigInteger, primary_key=True)\n",
    "    key1          = Column(String, nullable=False)\n",
    "    key2          = Column(String, nullable=False)\n",
    "    key3          = Column(String, nullable=False)\n",
    "    incidents     = relationship(\"Incident\", back_populates=\"locationdesc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetData(object):\n",
    "    def go(self, SESSION, start_year, end_year):\n",
    "        SQL_QUERY = \\\n",
    "            f'''\n",
    "                WITH\n",
    "                    max_severity AS (\n",
    "                        SELECT MAX(severity) AS severity\n",
    "                        FROM (\n",
    "                            SELECT SUM(crimetype.severity)/AVG(block.population) AS severity\n",
    "                            FROM incident\n",
    "                            INNER JOIN block ON incident.blockid = block.id INNER JOIN crimetype ON incident.crimetypeid = crimetype.id AND block.population > 0\n",
    "                            GROUP BY\n",
    "                                incident.blockid,\n",
    "                                incident.year,\n",
    "                                incident.month,\n",
    "                                incident.dow,\n",
    "                                incident.hour\n",
    "                        ) AS categories\n",
    "                    ),\n",
    "                    block_incidents AS (\n",
    "                        SELECT\n",
    "                            incident.blockid,\n",
    "                            incident.year,\n",
    "                            incident.month,\n",
    "                            incident.dow,\n",
    "                            incident.hour,\n",
    "                            SUM(crimetype.severity)/AVG(block.population) AS severity\n",
    "                        FROM incident\n",
    "                        INNER JOIN block ON incident.blockid = block.id\n",
    "                        INNER JOIN crimetype ON incident.crimetypeid = crimetype.id\n",
    "                            AND block.population > 0\n",
    "                            AND incident.cityid = 1\n",
    "                            AND incident.year >= {start_year}\n",
    "                            AND incident.year <= {end_year}\n",
    "                        GROUP BY\n",
    "                            incident.blockid,\n",
    "                            incident.year,\n",
    "                            incident.month,\n",
    "                            incident.dow,\n",
    "                            incident.hour\n",
    "                    )\n",
    "                SELECT\n",
    "                    block_incidents.blockid,\n",
    "                    block_incidents.year,\n",
    "                    block_incidents.month,\n",
    "                    block_incidents.dow,\n",
    "                    block_incidents.hour,\n",
    "                    block_incidents.severity/max_severity.severity AS severity\n",
    "                FROM block_incidents, max_severity        \n",
    "            '''\n",
    "        return SESSION.execute(text(SQL_QUERY)).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data, start_year, end_year):\n",
    "\n",
    "    NUM_BLOCKIDS = 801\n",
    "\n",
    "    X = np.zeros((NUM_BLOCKIDS, 24, 7*24+1))\n",
    "    y = np.zeros((NUM_BLOCKIDS, 12, 7*24))\n",
    "    blockid_dict = {}\n",
    "\n",
    "    # Create random array (BLOCKIDS) from 1-801 inclusive\n",
    "    # of length NUM_BLOCKIDS\n",
    "    BLOCKIDS = random.choices(list(range(1,802)), k=NUM_BLOCKIDS)\n",
    "    \n",
    "    for ind, blockid in enumerate(BLOCKIDS):\n",
    "        blockid_dict[blockid] = ind\n",
    "\n",
    "    # records is the list of rows we get from the query with this order:\n",
    "    #   blockid, year, month, dow, hour, risk\n",
    "    #   month is from 1 - 12\n",
    "\n",
    "    for r in data:\n",
    "        if r[0] in blockid_dict:\n",
    "            if r[1] == end_year:\n",
    "                # index into array  0-based month\n",
    "                # vvvvvvvvvvvvvvvv    vvvvvv\n",
    "                y[blockid_dict[r[0]], r[2]-1, 24*r[3]+r[4]] = float(r[5])\n",
    "                #                             ^^^^^^^^^^^^^   ^^^^\n",
    "                #                             hours since     risk\n",
    "                #                             beginning of\n",
    "                #                             week\n",
    "            else:\n",
    "                # month, year = get_month_year(datetime)\n",
    "                # index into array    year 0.....1   month   \n",
    "                # vvvvvvvvvvvvvvvv    vvvvvvvvvvvvv  vvvvvv\n",
    "                X[blockid_dict[r[0]], 12*(r[1]-start_year)+r[2]-1, 24*r[3]+r[4]] = float(r[5])\n",
    "                #                                                  ^^^^^^^^^^^^^   ^^^^\n",
    "                #                                                  hours since     risk\n",
    "                #                                                  beginning of\n",
    "                #                                                  week\n",
    "    \n",
    "    for i in range(24):\n",
    "        X[:, i, -1] = (start_year*12+i) / (2000 * 12)\n",
    "        \n",
    "    #     for i in range(12):\n",
    "    #         y[:, i, -1] = start_year*12+i\n",
    "    \n",
    "    #     for i in range(0, NUM_BLOCKIDS):\n",
    "    #         X[i, :, -1] = blockid_dict.get(i+1, 0)\n",
    "    #         y[i, :, -1] = blockid_dict.get(i+1, 0)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def session_scope():\n",
    "    \"\"\"Provide a transactional scope around a series of operations.\"\"\"\n",
    "\n",
    "    DB_URI  = config('DB_URI')\n",
    "    ENGINE  = create_engine(DB_URI)\n",
    "    Session = sessionmaker(bind=ENGINE)\n",
    "    SESSION = Session()\n",
    "    \n",
    "    try:\n",
    "        yield SESSION\n",
    "        SESSION.commit()\n",
    "    except:\n",
    "        SESSION.rollback()\n",
    "        raise\n",
    "    finally:\n",
    "        SESSION.close()\n",
    "\n",
    "\n",
    "def ready_data(training_start_year, training_end_year,\n",
    "               testing_start_year, testing_end_year):\n",
    "    with session_scope() as session:\n",
    "        training_data = GetData().go(session,\n",
    "                                     training_start_year,\n",
    "                                     training_end_year)\n",
    "        testing_data = GetData().go(session,\n",
    "                                     testing_start_year,\n",
    "                                     testing_end_year)\n",
    "        \n",
    "        X_train, y_train = process_data(training_data,\n",
    "                                        training_start_year, \n",
    "                                        training_end_year)\n",
    "        X_test, y_test = process_data(testing_data,\n",
    "                                      testing_start_year, \n",
    "                                      testing_end_year)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(801, 24, 169) (801, 12, 168) (801, 24, 169) (801, 12, 168)\n",
      "CPU times: user 3.06 s, sys: 285 ms, total: 3.34 s\n",
      "Wall time: 1min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# start month = 3, end_month = 2 (months are 0-indexed)\n",
    "#   X: 4/2017 -> 3/2019 actual date\n",
    "#   y: 4/2019 -> 3/2020 actual date\n",
    "#\n",
    "X_test_start_month = 0\n",
    "X_test_end_month   = 0\n",
    "X_test_start_year  = 2016\n",
    "X_test_end_year    = 2018\n",
    "X_train, X_test, y_train, y_test = ready_data(2015, 2017, X_test_start_year, X_test_end_year)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_output(y, y_pred, dataset_type):\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    plt.plot(np.arange(len(y.flatten())),\n",
    "                 y.flatten(), color='blue');\n",
    "    plt.plot(np.arange(len(y_pred.flatten())),\n",
    "                 y_pred.flatten(), color='red');\n",
    "    plt.xlabel('Hour since beginning of data', fontsize=16)\n",
    "    plt.ylabel('Risk', fontsize=18)\n",
    "    plt.title(dataset_type + ' dataset', fontsize=18)\n",
    "    plt.legend(labels=['risk', 'predicted risk'], prop={'size': 20})\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_validation_loss(epochs, history):\n",
    "    plt.plot(range(1, epochs), history.history['loss'])\n",
    "    plt.plot(range(1, epochs), history.history['val_loss'])\n",
    "    plt.title('model train vs validation loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Reshape, Dropout, LeakyReLU\n",
    "from keras.optimizers import Adam, SGD, RMSprop, Adagrad, Adadelta, Adamax, Nadam\n",
    "import keras.backend as K\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "random.seed(42)\n",
    "best_model = None\n",
    "best_mse   = None\n",
    "\n",
    "def perc_error(y_true, y_pred):\n",
    "    return 100.0 * K.mean((y_true - y_pred) / y_true)\n",
    "\n",
    "def create_model(learn_rate=0.01, \n",
    "                 momentum=0,\n",
    "                 opt_name='Adam',\n",
    "                 init_mode='uniform',\n",
    "                 activation='leakyrelu'\n",
    "                ):\n",
    "    data_dim    = 7*24+1   # All values in each hour of the week\n",
    "                           # averaged over each day for all weeks\n",
    "                           # of the month\n",
    "    timesteps   = 2 * 12   # Summed per month\n",
    "    batch_size  = 64\n",
    "    num_outputs = (7*24+1) * 12\n",
    "    \n",
    "    if activation == 'leakyrelu':\n",
    "        activation = LeakyReLU()\n",
    "    \n",
    "    # expected input data shape: (batch_size, timesteps, data_dim)\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(1024, input_shape=(24,169), activation=activation))  # (24, 170) input dot (170 x 1024)\n",
    "                                                                        # gives (24 x 1024) output\n",
    "    model.add(Dense(12*168, kernel_initializer=init_mode, activation=activation))  # (24 x 1024 input dot (1024 x (170/2)??))\n",
    "                                                                                   # gives (12 x 170) output\n",
    "    model.add(Reshape((12,168)))\n",
    "    \n",
    "    optimizer = { \n",
    "        'SGD':      SGD(lr=learn_rate, momentum=momentum),\n",
    "        'RMSprop':  RMSprop(lr=learn_rate),\n",
    "        'Adagrad':  Adagrad(lr=learn_rate),\n",
    "        'Adadelta': Adadelta(lr=learn_rate),\n",
    "        'Adam':     Adam(lr=learn_rate),\n",
    "        'Adamax':   Adamax(lr=learn_rate),\n",
    "        'Nadam':    Nadam(lr=learn_rate),\n",
    "    }\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer[opt_name])\n",
    "\n",
    "    return model\n",
    "\n",
    "def my_gridsearch(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    # GridSearchCV's fit method requires X which is 2D, y which is 1D.\n",
    "    # This is a problem for us since our X and y are 3D.\n",
    "    # Instead of GridSearchCV, we will create our own loop to\n",
    "    # search through the grid.\n",
    "    \n",
    "    global best_model, best_mse\n",
    "    \n",
    "    # \n",
    "    #     for epochs in range(5, 6, 5):\n",
    "    #         for lr in [0.00001, 0.0001, 0.001, 0.01, 0.1]:\n",
    "    #             for opt_name in ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']:\n",
    "    #                 for init_mode in ['uniform', 'lecun_uniform', 'normal', 'zero',\n",
    "    #                                   'glorot_normal', 'glorot_uniform', 'he_normal',\n",
    "    #                                   'he_uniform']:\n",
    "    #                     for activation in ['relu']:\n",
    "    batch_size = 64\n",
    "    for epochs in range(5, 6, 5):\n",
    "        for lr in [1e-4]:\n",
    "            for opt_name in ['Adam']:\n",
    "                for init_mode in ['uniform', 'lecun_uniform', 'zero']:\n",
    "                    for activation in ['relu', 'leakyrelu']:\n",
    "\n",
    "                        print('>'*80)\n",
    "                        print('epochs:', epochs, '   lr:', lr, \\\n",
    "                              '   opt:', opt_name, \\\n",
    "                              '   init:', init_mode, \\\n",
    "                              '   act:', 'relu')\n",
    "                        print('>'*80)\n",
    "\n",
    "                        model = create_model(learn_rate=lr, \n",
    "                                             opt_name=opt_name,\n",
    "                                             init_mode=init_mode,\n",
    "                                             activation=activation)\n",
    "\n",
    "                        history = model.fit(X_train, y_train,\n",
    "                                            batch_size=batch_size, \n",
    "                                            epochs=epochs,\n",
    "                                            validation_data=(X_test, y_test))\n",
    "\n",
    "                        mse = model.evaluate(X_test, y_test,\n",
    "                                             batch_size=batch_size)\n",
    "                        if (best_mse is None) or \\\n",
    "                            (mse < best_mse):\n",
    "                            best_mse = mse\n",
    "                            best_model = model\n",
    "                            print('best_model:', best_model)\n",
    "                        \n",
    "                        print('<'*80)\n",
    "                        print('epochs:', epochs, \\\n",
    "                              '   lr:', lr, \\\n",
    "                              '   opt:', opt_name, \\\n",
    "                              '   init:', init_mode, \\\n",
    "                              '   act:', 'relu', \\\n",
    "                              ' Test MSE:', mse \\\n",
    "                        )\n",
    "                        print('<'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_24 (LSTM)               (None, 1024)              4890624   \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 2016)              2066400   \n",
      "_________________________________________________________________\n",
      "reshape_24 (Reshape)         (None, 12, 168)           0         \n",
      "=================================================================\n",
      "Total params: 6,957,024\n",
      "Trainable params: 6,957,024\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 801 samples, validate on 801 samples\n",
      "Epoch 1/15\n",
      "801/801 [==============================] - 2s 2ms/step - loss: 3.1310e-07 - val_loss: 2.9819e-08\n",
      "Epoch 2/15\n",
      "801/801 [==============================] - 2s 2ms/step - loss: 3.0151e-07 - val_loss: 2.5360e-08\n",
      "Epoch 3/15\n",
      "801/801 [==============================] - 2s 2ms/step - loss: 2.9674e-07 - val_loss: 5.4592e-08\n",
      "Epoch 4/15\n",
      "801/801 [==============================] - 2s 2ms/step - loss: 3.1724e-07 - val_loss: 3.1705e-08\n",
      "Epoch 5/15\n",
      "801/801 [==============================] - 2s 2ms/step - loss: 2.9932e-07 - val_loss: 2.6495e-08\n",
      "Epoch 6/15\n",
      "801/801 [==============================] - 2s 2ms/step - loss: 3.0097e-07 - val_loss: 2.4987e-08\n",
      "Epoch 7/15\n",
      "801/801 [==============================] - 2s 2ms/step - loss: 2.9980e-07 - val_loss: 2.3822e-08\n",
      "Epoch 8/15\n",
      "801/801 [==============================] - 2s 2ms/step - loss: 3.0116e-07 - val_loss: 2.4620e-08\n",
      "Epoch 9/15\n",
      "801/801 [==============================] - 2s 2ms/step - loss: 3.0135e-07 - val_loss: 2.3258e-08\n",
      "Epoch 10/15\n",
      "801/801 [==============================] - 2s 2ms/step - loss: 2.9997e-07 - val_loss: 2.6239e-08\n",
      "Epoch 11/15\n",
      "801/801 [==============================] - 2s 2ms/step - loss: 3.0474e-07 - val_loss: 2.9490e-08\n",
      "Epoch 12/15\n",
      "801/801 [==============================] - 2s 2ms/step - loss: 3.0143e-07 - val_loss: 2.4949e-08\n",
      "Epoch 13/15\n",
      "801/801 [==============================] - 2s 2ms/step - loss: 3.0003e-07 - val_loss: 5.9973e-08\n",
      "Epoch 14/15\n",
      "801/801 [==============================] - 2s 2ms/step - loss: 3.1598e-07 - val_loss: 3.7293e-08\n",
      "Epoch 15/15\n",
      "801/801 [==============================] - 2s 2ms/step - loss: 3.1403e-07 - val_loss: 3.5199e-08\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe775491f98>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=64, epochs=15, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epochs: 5    lr: 0.0001    opt: Adam    init: uniform    act: relu\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected reshape_10 to have shape (12, 169) but got array with shape (12, 168)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-ebbff661cba0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'timeit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"my_gridsearch(X_train, y_train, X_test, y_test)\\nprint('Best test MSE:', best_mse)\\nbest_model.summary()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2165\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2166\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2167\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2168\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-61>\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1096\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m                 \u001b[0mnumber\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m                 \u001b[0mtime_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtime_number\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mtiming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgcold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<magic-timeit>\u001b[0m in \u001b[0;36minner\u001b[0;34m(_it, _timer)\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-941af7f99f6d>\u001b[0m in \u001b[0;36mmy_gridsearch\u001b[0;34m(X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[1;32m     84\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                                             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                                             validation_data=(X_test, y_test))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                         mse = model.evaluate(X_test, y_test,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected reshape_10 to have shape (12, 169) but got array with shape (12, 168)"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "my_gridsearch(X_train, y_train, X_test, y_test)\n",
    "print('Best test MSE:', best_mse)\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = best_model.predict(X_train).flatten()\n",
    "y_test_pred = best_model.predict(X_test).flatten()\n",
    "\n",
    "plot_output(y_train, y_train_pred, 'Training')\n",
    "plot_output(y_test, y_test_pred, 'Testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_y_stats(s, y):\n",
    "    num_spaces = 2\n",
    "    if isinstance(y, pd.core.series.Series):\n",
    "        y_flat = y\n",
    "    elif isinstance(y, np.ndarray):\n",
    "        y_flat = y.flatten()\n",
    "    else:\n",
    "        raise ValueError('Could not process type:', type(y))\n",
    "        \n",
    "    print(s)\n",
    "    print(' ' * num_spaces, 'min: ', min(y_flat))\n",
    "    print(' ' * num_spaces, 'max: ', max(y_flat))\n",
    "    print(' ' * num_spaces, 'mean:', np.mean(y_flat))\n",
    "    print(' ' * num_spaces, 'std: ', np.std(y_flat))\n",
    "    return min(y_flat), max(y_flat), np.mean(y_flat), np.std(y_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, _, y_train_std = print_y_stats('y_train:', y_train)\n",
    "print()\n",
    "print_y_stats('y_test:', y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store predictions in DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_predictions_in_db(y_pred, X):\n",
    "    \n",
    "    # Put predictions into pandas DataFrame with corresponding block id\n",
    "    predictions = pd.DataFrame([[x] for x in list(block_ids)], columns=[\"id\"])\n",
    "\n",
    "    block_ids = {}\n",
    "    for i in range(X.shape[0]):\n",
    "        block_ids[X.loc[i, 'blockid']] = i\n",
    "\n",
    "    # Put predictions into pandas DataFrame with corresponding block id\n",
    "    predictions = pd.DataFrame([[x] for x in list(block_ids)], columns=[\"id\"])\n",
    "    predictions.loc[:, \"prediction\"] = predictions[\"id\"].apply(lambda x: y_pred[block_ids[x],:].astype(np.float64).tobytes().hex())\n",
    "    predictions.loc[:, \"month\"] = end_month\n",
    "    predictions.loc[:, \"year\"] = end_year\n",
    "    predictions.to_csv(\"predictions.csv\", index=False)\n",
    "\n",
    "    # Query SQL\n",
    "    query_commit_predictions = \"\"\"\n",
    "    CREATE TEMPORARY TABLE temp_predictions (\n",
    "        id SERIAL PRIMARY KEY,\n",
    "        prediction TEXT,\n",
    "        month INTEGER,\n",
    "        year INTEGER\n",
    "    );\n",
    "\n",
    "    COPY temp_predictions (id, prediction, month, year) FROM STDIN DELIMITER ',' CSV HEADER;\n",
    "\n",
    "    UPDATE block\n",
    "    SET \n",
    "        prediction = DECODE(temp_predictions.prediction, 'hex'),\n",
    "        month = temp_predictions.month,\n",
    "        year = temp_predictions.year \n",
    "    FROM temp_predictions\n",
    "    WHERE block.id = temp_predictions.id;\n",
    "\n",
    "    DROP TABLE temp_predictions;\n",
    "    \"\"\"\n",
    "\n",
    "    # Open saved predictions and send to database using above query\n",
    "    with open(\"predictions.csv\", \"r\") as f:\n",
    "        print(\"SENDING TO DB\")\n",
    "        RAW_CONN = create_engine(DB_URI).raw_connection()\n",
    "        cursor = RAW_CONN.cursor()\n",
    "        cursor.copy_expert(query_commit_predictions, f)\n",
    "        RAW_CONN.commit()\n",
    "        RAW_CONN.close()\n",
    "    os.remove(\"predictions.csv\")\n",
    "\n",
    "    for r in SESSION.execute(\"SELECT ENCODE(prediction::BYTEA, 'hex'), id FROM block WHERE prediction IS NOT NULL LIMIT 5;\").fetchall():\n",
    "        print(np.frombuffer(bytes.fromhex(r[0]), dtype=np.float64).reshape((12,7,24)))\n",
    "        print(X[block_ids[int(r[1])], :].reshape((12,7,24)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_predictions_in_db(y_test_pred, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try filtering values that are 20 standard deviations above the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = np.std(y_train)\n",
    "y_train_filtered = y_train[y_train < 20*std] # Remove all values larger than 20 standard deviations\n",
    "\n",
    "std = np.std(y_test)\n",
    "y_test_filtered = y_test[y_test < 20*std]   # Remove all values larger than 20 standard deviations\n",
    "\n",
    "print('Number of values filtered from y_train:', len(y_train[y_train > 20*std]))\n",
    "print('Number of values filtered from y_test:', len(y_test[y_test > 20*std]))\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "plt.plot(y_train_filtered, color='blue');\n",
    "plt.plot(y_test_filtered, color='red');\n",
    "plt.legend(labels=['training set data', 'testing set data'], prop={'size': 20})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With our threshold of 20 * std, we have removed 13 points from y_train and 12 from y_test. This is out of 1.6 million points, so they were defintely outliers. Let's run the prediction again with the updated y values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_updated = np.where(y_train.flatten() < 20*std, y_train.flatten(), [0.]*len(y_train.flatten()))\n",
    "y_test_updated  = np.where(y_test.flatten() < 20*std, y_test.flatten(), [0.]*len(y_test.flatten()))\n",
    "\n",
    "predict(X_train, \n",
    "        y_train_updated.reshape((801, 12, 168)), \n",
    "        X_test, \n",
    "        y_test_updated.reshape((801, 12, 168)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_updated_pred = model.predict(X_train).flatten()\n",
    "y_test_updated_pred = model.predict(X_test).flatten()\n",
    "\n",
    "plot_output(y_train_updated, y_train_updated_pred, 'Training')\n",
    "plot_output(y_test_updated, y_test_updated_pred, 'Testing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of y-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "pd.Series(y_train_updated).hist(bins=np.arange(0.0001, 0.002, 0.0002));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "pd.Series(y_test_updated).hist(bins=np.arange(0.0001, 0.002, 0.0002));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This data seems reasonable, although it is a little lopsided. Still, it shouldn't cause the neural network to give us the large error that it is giving. Let's try giving the network the full set of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the full set of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetDataFull(object):\n",
    "    def go(self, SESSION, start_year, end_year):\n",
    "        SQL_QUERY = \\\n",
    "            f'''\n",
    "            SELECT incident.blockid, \n",
    "                    incident.year, \n",
    "                    incident.month, \n",
    "                    incident.dow, \n",
    "                    incident.hour,\n",
    "                    SUM(crimetype.severity)/AVG(block.population) AS severity\n",
    "            FROM incident\n",
    "            INNER JOIN block ON incident.blockid = block.id INNER JOIN crimetype ON incident.crimetypeid = crimetype.id AND block.population > 0\n",
    "                AND block.population > 0\n",
    "                AND severity > 0\n",
    "                AND incident.cityid = 1\n",
    "                AND incident.year >= {start_year}\n",
    "                AND incident.year <= {end_year}\n",
    "            GROUP BY\n",
    "                incident.blockid,\n",
    "                incident.year,\n",
    "                incident.month,\n",
    "                incident.dow,\n",
    "                incident.hour\n",
    "            '''\n",
    "        return SESSION.execute(text(SQL_QUERY)).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def days_in_month(year, month):\n",
    "    p = pd.Period(f'{year}-{month}-1')\n",
    "    return p.days_in_month\n",
    "\n",
    "def day_of_week(dt):\n",
    "    return dt.weekday()\n",
    "\n",
    "def create_arrays(blockids, start_year, end_year):\n",
    "    idx = 0\n",
    "    X_blockid, X_year, X_month, X_dow, X_hour, X_risk = [], [], [], [], [], []\n",
    "    for blockid in blockids:\n",
    "        for year in range(start_year, end_year + 1):\n",
    "            for month in range(1, 12 + 1):      # month range is 1-12\n",
    "                for day in range(1, days_in_month(year, month) + 1):\n",
    "                    for hour in range(24):      # hour range is 0-23\n",
    "                        X_blockid.append(blockid)\n",
    "                        X_year.append(year)\n",
    "                        X_month.append(month)\n",
    "                        X_dow.append(day_of_week(datetime(year, month, day)))\n",
    "                        X_hour.append(hour)\n",
    "                        X_risk.append(0.0)\n",
    "                        idx += 1\n",
    "    \n",
    "    X = pd.DataFrame({'blockid':  X_blockid,\n",
    "                      'year':     X_year,\n",
    "                      'month':    X_month,\n",
    "                      'dow':      X_dow,\n",
    "                      'hour':     X_hour,\n",
    "                      'risk':     X_risk})\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_full(data, start_year, end_year):\n",
    "\n",
    "    def remove_outliers_from_risk(risk):\n",
    "        std = np.std(risk)\n",
    "        risk = np.where(risk < 20*std, \n",
    "                     risk, \n",
    "                     [0.]*len(risk)).reshape(risk.shape)\n",
    "\n",
    "        return risk\n",
    "    \n",
    "    NUM_BLOCKIDS = 801\n",
    "    \n",
    "    delta_years = end_year - start_year + 1\n",
    "    \n",
    "    blockid_dict = {}\n",
    "\n",
    "    # Create random array (BLOCKIDS) from 1-801 inclusive\n",
    "    # of length NUM_BLOCKIDS\n",
    "    BLOCKIDS = random.choices(list(range(1,802)), k=NUM_BLOCKIDS)\n",
    "    \n",
    "    for ind, blockid in enumerate(BLOCKIDS):\n",
    "        blockid_dict[blockid] = ind\n",
    "\n",
    "    blockids = list(blockid_dict.values())\n",
    "    X = create_arrays(blockids, start_year, end_year)\n",
    "\n",
    "    # records is the list of rows we get from the query with this order:\n",
    "    #   blockid, year, month, dow, hour, risk\n",
    "    #   month is from 1 - 12\n",
    "\n",
    "    X1 = []\n",
    "    for r in data:\n",
    "        if r[0] in blockid_dict:            \n",
    "            X1.append((r[0], r[1], r[2], r[3], r[4], r[5]))\n",
    "\n",
    "    X1 = pd.DataFrame(data=X1,\n",
    "                      columns=['blockid', 'year', 'month', 'dow', 'hour','risk2'])\n",
    "    X = pd.merge(X, X1, \n",
    "                 how='left',\n",
    "                 left_on=['blockid', 'year', 'month', 'dow', 'hour'],\n",
    "                 right_on=['blockid', 'year', 'month', 'dow', 'hour']\n",
    "                )\n",
    "    X['all_risk'] = X.risk.astype(float) + X.risk2.astype(float)\n",
    "    X = X.drop(columns=['risk', 'risk2']) \\\n",
    "         .rename(mapper={'all_risk': 'risk'}, axis=1)\n",
    "    \n",
    "    y = X['risk'].copy()\n",
    "    X = X.drop(columns=['risk']).copy()\n",
    "    y = remove_outliers_from_risk(y)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ready_data_full(training_start_year, training_end_year,\n",
    "                    testing_start_year, testing_end_year):\n",
    "    with session_scope() as session:\n",
    "        training_data = GetDataFull().go(session,\n",
    "                                         training_start_year,\n",
    "                                         training_end_year)\n",
    "        testing_data = GetDataFull().go(session,\n",
    "                                         testing_start_year,\n",
    "                                         testing_end_year)\n",
    "        X_train, y_train = process_data_full(training_data,\n",
    "                                             training_start_year, \n",
    "                                             training_end_year)\n",
    "        X_test, y_test = process_data_full(testing_data,\n",
    "                                           testing_start_year, \n",
    "                                           testing_end_year)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_train, X_test, y_train, y_test = ready_data_full(2015, 2016, 2017, 2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_full(X_train, y_train, X_test, y_test):\n",
    "#     data_dim    = 5 # 7 * 24   # All values in each hour of the week\n",
    "#     timesteps   = 2 * 12   # Summed per month\n",
    "#     batch_size  = 64\n",
    "#     num_outputs = 7 * 24 * 12\n",
    "\n",
    "#     # expected input data shape: (batch_size, timesteps, data_dim)\n",
    "#     model = Sequential()\n",
    "#     model.add(LSTM(1024, return_sequences=True, \n",
    "# #                    input_shape=(timesteps, # 24\n",
    "# #                                 data_dim), # 168\n",
    "#                    activation='relu',\n",
    "#                    kernel_initializer='random_uniform',\n",
    "#                    bias_initializer='zeros'\n",
    "#                   )\n",
    "#              )\n",
    "#     model.add(LSTM(1024, return_sequences=True, input_shape=(timesteps, data_dim)))\n",
    "#     model.add(LSTM(128, input_shape=(timesteps, data_dim), activation='relu'))\n",
    "#     model.add(Dense(num_outputs, activation='relu'))\n",
    "#     model.add(Reshape((12, 7 * 24)))\n",
    "\n",
    "#     model.compile(loss='mean_squared_error',\n",
    "#                    optimizer=Adam(lr=0.1))\n",
    "\n",
    "#     history = model.fit(X_train, y_train,\n",
    "#                         batch_size=batch_size, epochs=10,\n",
    "#                         validation_data=(X_test, y_test))\n",
    "\n",
    "#     mse = model.evaluate(X_test, y_test,\n",
    "#                          batch_size=batch_size)\n",
    "#     print('Test MSE:', mse)\n",
    "#     return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# history, model = predict_full(X_train, y_train, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
