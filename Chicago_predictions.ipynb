{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pip in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (19.1.1)\n",
      "Requirement already satisfied: python-decouple in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (3.1)\n",
      "Requirement already satisfied: geoalchemy2 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (0.6.2)\n",
      "Requirement already satisfied: SQLAlchemy>=0.8 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from geoalchemy2) (1.2.11)\n",
      "Requirement already satisfied: shapely in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (1.6.4.post2)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (1.1.0)\n",
      "Requirement already satisfied: hyperas in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (0.4.1)\n",
      "Requirement already satisfied: entrypoints in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from hyperas) (0.2.3)\n",
      "Requirement already satisfied: keras in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from hyperas) (2.2.4)\n",
      "Requirement already satisfied: hyperopt in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from hyperas) (0.1.2)\n",
      "Requirement already satisfied: jupyter in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from hyperas) (1.0.0)\n",
      "Requirement already satisfied: nbformat in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from hyperas) (4.4.0)\n",
      "Requirement already satisfied: nbconvert in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from hyperas) (5.4.1)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from keras->hyperas) (1.15.4)\n",
      "Requirement already satisfied: h5py in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from keras->hyperas) (2.8.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from keras->hyperas) (1.11.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from keras->hyperas) (1.0.7)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from keras->hyperas) (3.12)\n",
      "Requirement already satisfied: scipy>=0.14 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from keras->hyperas) (1.1.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from keras->hyperas) (1.0.9)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from hyperopt->hyperas) (4.32.0)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from hyperopt->hyperas) (2.1)\n",
      "Requirement already satisfied: future in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from hyperopt->hyperas) (0.17.1)\n",
      "Requirement already satisfied: pymongo in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from hyperopt->hyperas) (3.8.0)\n",
      "Requirement already satisfied: notebook in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from jupyter->hyperas) (5.5.0)\n",
      "Requirement already satisfied: qtconsole in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from jupyter->hyperas) (4.3.1)\n",
      "Requirement already satisfied: jupyter-console in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from jupyter->hyperas) (5.2.0)\n",
      "Requirement already satisfied: ipykernel in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from jupyter->hyperas) (4.8.2)\n",
      "Requirement already satisfied: ipywidgets in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from jupyter->hyperas) (7.4.0)\n",
      "Requirement already satisfied: ipython_genutils in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from nbformat->hyperas) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=4.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from nbformat->hyperas) (4.3.2)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from nbformat->hyperas) (2.6.0)\n",
      "Requirement already satisfied: jupyter_core in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from nbformat->hyperas) (4.4.0)\n",
      "Requirement already satisfied: mistune>=0.8.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from nbconvert->hyperas) (0.8.3)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from nbconvert->hyperas) (2.10)\n",
      "Requirement already satisfied: pygments in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from nbconvert->hyperas) (2.2.0)\n",
      "Requirement already satisfied: bleach in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from nbconvert->hyperas) (2.1.3)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from nbconvert->hyperas) (1.4.2)\n",
      "Requirement already satisfied: testpath in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from nbconvert->hyperas) (0.3.1)\n",
      "Requirement already satisfied: defusedxml in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from nbconvert->hyperas) (0.5.0)\n",
      "Requirement already satisfied: decorator>=4.1.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from networkx->hyperopt->hyperas) (4.3.0)\n",
      "Requirement already satisfied: tornado>=4 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from notebook->jupyter->hyperas) (5.0.2)\n",
      "Requirement already satisfied: jupyter-client>=5.2.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from notebook->jupyter->hyperas) (5.2.3)\n",
      "Requirement already satisfied: terminado>=0.8.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from notebook->jupyter->hyperas) (0.8.1)\n",
      "Requirement already satisfied: pyzmq>=17 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from notebook->jupyter->hyperas) (17.0.0)\n",
      "Requirement already satisfied: Send2Trash in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from notebook->jupyter->hyperas) (1.5.0)\n",
      "Requirement already satisfied: ipython in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from jupyter-console->jupyter->hyperas) (6.4.0)\n",
      "Requirement already satisfied: prompt_toolkit<2.0.0,>=1.0.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from jupyter-console->jupyter->hyperas) (1.0.15)\n",
      "Requirement already satisfied: widgetsnbextension~=3.4.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from ipywidgets->jupyter->hyperas) (3.4.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from jinja2->nbconvert->hyperas) (1.0)\n",
      "Requirement already satisfied: html5lib!=1.0b1,!=1.0b2,!=1.0b3,!=1.0b4,!=1.0b5,!=1.0b6,!=1.0b7,!=1.0b8,>=0.99999999pre in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from bleach->nbconvert->hyperas) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from jupyter-client>=5.2.0->notebook->jupyter->hyperas) (2.7.3)\n",
      "Requirement already satisfied: simplegeneric>0.8 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from ipython->jupyter-console->jupyter->hyperas) (0.8.1)\n",
      "Requirement already satisfied: setuptools>=18.5 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from ipython->jupyter-console->jupyter->hyperas) (39.1.0)\n",
      "Requirement already satisfied: pickleshare in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from ipython->jupyter-console->jupyter->hyperas) (0.7.4)\n",
      "Requirement already satisfied: backcall in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from ipython->jupyter-console->jupyter->hyperas) (0.1.0)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from ipython->jupyter-console->jupyter->hyperas) (4.5.0)\n",
      "Requirement already satisfied: jedi>=0.10 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from ipython->jupyter-console->jupyter->hyperas) (0.12.0)\n",
      "Requirement already satisfied: wcwidth in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from prompt_toolkit<2.0.0,>=1.0.0->jupyter-console->jupyter->hyperas) (0.1.7)\n",
      "Requirement already satisfied: webencodings in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from html5lib!=1.0b1,!=1.0b2,!=1.0b3,!=1.0b4,!=1.0b5,!=1.0b6,!=1.0b7,!=1.0b8,>=0.99999999pre->bleach->nbconvert->hyperas) (0.5.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from pexpect; sys_platform != \"win32\"->ipython->jupyter-console->jupyter->hyperas) (0.5.2)\n",
      "Requirement already satisfied: parso>=0.2.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from jedi>=0.10->ipython->jupyter-console->jupyter->hyperas) (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install python-decouple\n",
    "!pip install geoalchemy2\n",
    "!pip install shapely\n",
    "!pip install scipy\n",
    "!pip install hyperas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine, func, text\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from decouple import config\n",
    "from shapely import wkb, wkt\n",
    "from shapely.geometry import Point\n",
    "from geoalchemy2.shape import to_shape \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from hyperas.distributions import uniform\n",
    "\n",
    "\n",
    "\n",
    "# ----------- TODO: Issues with importing imbalance-learn library\n",
    "# from imblearn.over_sampling import RandomOverSampler\n",
    "# from imblearn.under_sampling import NearMiss\n",
    "# from imblearn.under_sampling import (RandomUnderSampler,\n",
    "#                                      ClusterCentroids,\n",
    "#                                      TomekLinks,\n",
    "#                                      NeighbourhoodCleaningRule,\n",
    "#                                      NearMiss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Contains models for DB.\"\"\"\n",
    "\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy import Column, BigInteger, Integer, String, DateTime, ForeignKey, Float\n",
    "from sqlalchemy.orm import relationship\n",
    "from geoalchemy2 import Geometry\n",
    "\n",
    "\n",
    "BASE = declarative_base()\n",
    "\n",
    "\n",
    "class City(BASE):\n",
    "    \"\"\"City model for DB. Has information of cities.\"\"\"\n",
    "    __tablename__ = 'city'\n",
    "    id            = Column(BigInteger, primary_key=True)\n",
    "    city          = Column(String, unique=False, nullable=False)\n",
    "    state         = Column(String, unique=False, nullable=True)\n",
    "    country       = Column(String, unique=False, nullable=False)\n",
    "    location      = Column(Geometry(geometry_type='POINT'), nullable=False)\n",
    "    blocks        = relationship(\"Blocks\", back_populates=\"city\")\n",
    "    zipcodes      = relationship(\"ZipcodeGeom\", back_populates=\"city\")\n",
    "    incidents     = relationship(\"Incident\", back_populates=\"city\")\n",
    "\n",
    "\n",
    "class Blocks(BASE):\n",
    "    \"\"\"Block model for DB. Has information of city blocks for a related city\n",
    "        id.\"\"\"\n",
    "    __tablename__ = 'block'\n",
    "    id            = Column(BigInteger, primary_key=True)\n",
    "    cityid        = Column(BigInteger, ForeignKey('city.id'), nullable=False)\n",
    "    shape         = Column(Geometry(geometry_type='MULTIPOLYGON'), nullable=False)\n",
    "    population    = Column(Integer, nullable=False)\n",
    "    city          = relationship(\"City\", back_populates=\"blocks\")\n",
    "    incidents     = relationship(\"Incident\", back_populates=\"block\")\n",
    "\n",
    "class ZipcodeGeom(BASE):\n",
    "    \"\"\"Zipcode geometry model for DB. Has information of zipcodes and related\n",
    "        city id.\"\"\"\n",
    "    __tablename__ = 'zipcodegeom'\n",
    "    id            = Column(BigInteger, primary_key=True)\n",
    "    cityid        = Column(BigInteger, ForeignKey('city.id'), nullable=False)\n",
    "    zipcode       = Column(String, nullable=False, unique=True)\n",
    "    shape         = Column(Geometry(geometry_type='MULTIPOLYGON'), nullable=False)\n",
    "    city          = relationship(\"City\", back_populates=\"zipcodes\")\n",
    "\n",
    "class Incident(BASE):\n",
    "    \"\"\"Incident model for DB. Has information of a specific crime, including\n",
    "        where it took place, when it took place, and the type of crime that\n",
    "        occurred.\"\"\"\n",
    "    __tablename__ = 'incident'\n",
    "    id            = Column(BigInteger, primary_key=True)\n",
    "    crimetypeid   = Column(BigInteger, ForeignKey('crimetype.id'), nullable=False)\n",
    "    locdescid     = Column(BigInteger, ForeignKey('locdesctype.id'), nullable=False)\n",
    "    cityid        = Column(BigInteger, ForeignKey('city.id'), nullable=False)\n",
    "    blockid       = Column(BigInteger, ForeignKey('block.id'), nullable=False)\n",
    "    location      = Column(Geometry(geometry_type='POINT'), nullable=False)\n",
    "    datetime      = Column(DateTime, nullable=False)\n",
    "    hour          = Column(Integer, nullable=False)\n",
    "    dow           = Column(Integer, nullable=False)\n",
    "    month         = Column(Integer, nullable=False)\n",
    "    year          = Column(Integer, nullable=False)\n",
    "    city          = relationship(\"City\", back_populates=\"incidents\")\n",
    "    block         = relationship(\"Blocks\", back_populates=\"incidents\")\n",
    "    crimetype     = relationship(\"CrimeType\", back_populates=\"incidents\")\n",
    "    locationdesc  = relationship(\"LocationDescriptionType\", back_populates=\"incidents\")\n",
    "\n",
    "class CrimeType(BASE):\n",
    "    \"\"\"CrimeType model for DB. Has information of the types of crime, including\n",
    "        a general description and the numerical severity of the crime.\"\"\"\n",
    "    __tablename__ = 'crimetype'\n",
    "    id            = Column(BigInteger, primary_key=True)\n",
    "    category      = Column(String, unique=True, nullable=False)\n",
    "    severity      = Column(Integer, nullable=False)\n",
    "    incidents     = relationship(\"Incident\", back_populates=\"crimetype\")\n",
    "\n",
    "\n",
    "class LocationDescriptionType(BASE):\n",
    "    \"\"\"Location description model for DB. Has information on the type of\n",
    "        location that the crime took place.\"\"\"\n",
    "    __tablename__ = 'locdesctype'\n",
    "    id            = Column(BigInteger, primary_key=True)\n",
    "    key1          = Column(String, nullable=False)\n",
    "    key2          = Column(String, nullable=False)\n",
    "    key3          = Column(String, nullable=False)\n",
    "    incidents     = relationship(\"Incident\", back_populates=\"locationdesc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetData(object):\n",
    "    def go(self, SESSION, start_year, end_year):\n",
    "        SQL_QUERY = \\\n",
    "            f'''\n",
    "                WITH\n",
    "                    max_severity AS (\n",
    "                        SELECT MAX(severity) AS severity\n",
    "                        FROM (\n",
    "                            SELECT SUM(crimetype.severity)/AVG(block.population) AS severity\n",
    "                            FROM incident\n",
    "                            INNER JOIN block ON incident.blockid = block.id INNER JOIN crimetype ON incident.crimetypeid = crimetype.id AND block.population > 0\n",
    "                            GROUP BY\n",
    "                                incident.blockid,\n",
    "                                incident.year,\n",
    "                                incident.month,\n",
    "                                incident.dow,\n",
    "                                incident.hour\n",
    "                        ) AS categories\n",
    "                    ),\n",
    "                    block_incidents AS (\n",
    "                        SELECT\n",
    "                            incident.blockid,\n",
    "                            incident.year,\n",
    "                            incident.month,\n",
    "                            incident.dow,\n",
    "                            incident.hour,\n",
    "                            SUM(crimetype.severity)/AVG(block.population) AS severity\n",
    "                        FROM incident\n",
    "                        INNER JOIN block ON incident.blockid = block.id\n",
    "                        INNER JOIN crimetype ON incident.crimetypeid = crimetype.id\n",
    "                            AND block.population > 0\n",
    "                            AND incident.cityid = 1\n",
    "                            AND incident.year >= {start_year}\n",
    "                            AND incident.year <= {end_year}\n",
    "                        GROUP BY\n",
    "                            incident.blockid,\n",
    "                            incident.year,\n",
    "                            incident.month,\n",
    "                            incident.dow,\n",
    "                            incident.hour\n",
    "                    )\n",
    "                SELECT\n",
    "                    block_incidents.blockid,\n",
    "                    block_incidents.year,\n",
    "                    block_incidents.month,\n",
    "                    block_incidents.dow,\n",
    "                    block_incidents.hour,\n",
    "                    block_incidents.severity/max_severity.severity AS severity\n",
    "                FROM block_incidents, max_severity        \n",
    "            '''\n",
    "        return SESSION.execute(text(SQL_QUERY)).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data, start_year, end_year, blockid_dict):\n",
    "\n",
    "    X = np.zeros((len(blockid_dict), 24, 7*24+1))\n",
    "    y = np.zeros((len(blockid_dict), 12, 7*24))\n",
    "    \n",
    "    # records is the list of rows we get from the query with this order:\n",
    "    #   blockid, year, month, dow, hour, risk\n",
    "    #   month is from 1 - 12\n",
    "\n",
    "    for r in data:\n",
    "        if r[0] in blockid_dict:\n",
    "            if r[1] == end_year:\n",
    "                # index into array  0-based month\n",
    "                # vvvvvvvvvvvvvvvv    vvvvvv\n",
    "                y[blockid_dict[r[0]], r[2]-1, 24*r[3]+r[4]] = float(r[5])\n",
    "                #                             ^^^^^^^^^^^^^   ^^^^\n",
    "                #                             hours since     risk\n",
    "                #                             beginning of\n",
    "                #                             week\n",
    "            else:\n",
    "                # month, year = get_month_year(datetime)\n",
    "                # index into array    year 0.....1   month   \n",
    "                # vvvvvvvvvvvvvvvv    vvvvvvvvvvvvv  vvvvvv\n",
    "                X[blockid_dict[r[0]], 12*(r[1]-start_year)+r[2]-1, 24*r[3]+r[4]] = float(r[5])\n",
    "                #                                                  ^^^^^^^^^^^^^   ^^^^\n",
    "                #                                                  hours since     risk\n",
    "                #                                                  beginning of\n",
    "                #                                                  week\n",
    "    \n",
    "    for i in range(24):\n",
    "        X[:, i, -1] = (start_year*12+i) / (2000 * 12)\n",
    "        \n",
    "    #     for i in range(12):\n",
    "    #         y[:, i, -1] = start_year*12+i\n",
    "    \n",
    "    #     for i in range(0, NUM_BLOCKIDS):\n",
    "    #         X[i, :, -1] = blockid_dict.get(i+1, 0)\n",
    "    #         y[i, :, -1] = blockid_dict.get(i+1, 0)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def session_scope():\n",
    "    \"\"\"Provide a transactional scope around a series of operations.\"\"\"\n",
    "\n",
    "    DB_URI  = config('DB_URI')\n",
    "    ENGINE  = create_engine(DB_URI)\n",
    "    Session = sessionmaker(bind=ENGINE)\n",
    "    SESSION = Session()\n",
    "    \n",
    "    try:\n",
    "        yield SESSION\n",
    "        SESSION.commit()\n",
    "    except:\n",
    "        SESSION.rollback()\n",
    "        raise\n",
    "    finally:\n",
    "        SESSION.close()\n",
    "\n",
    "\n",
    "def ready_data(training_start_year, training_end_year, train_blockid_dict,\n",
    "               testing_start_year, testing_end_year, test_blockid_dict):\n",
    "    with session_scope() as session:\n",
    "        training_data = GetData().go(session,\n",
    "                                     training_start_year,\n",
    "                                     training_end_year)\n",
    "        testing_data = GetData().go(session,\n",
    "                                     testing_start_year,\n",
    "                                     testing_end_year)\n",
    "        \n",
    "        X_train, y_train = process_data(training_data,\n",
    "                                        training_start_year, \n",
    "                                        training_end_year,\n",
    "                                        train_blockid_dict)\n",
    "        X_test, y_test = process_data(testing_data,\n",
    "                                      testing_start_year, \n",
    "                                      testing_end_year,\n",
    "                                      test_blockid_dict)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 24, 169) (800, 12, 168) (800, 24, 169) (800, 12, 168)\n",
      "CPU times: user 6.89 s, sys: 676 ms, total: 7.56 s\n",
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# start month = 3, end_month = 2 (months are 0-indexed)\n",
    "#   X: 4/2017 -> 3/2019 actual date\n",
    "#   y: 4/2019 -> 3/2020 actual date\n",
    "#\n",
    "X_test_start_month = 0\n",
    "X_test_end_month   = 0\n",
    "X_test_start_year  = 2016\n",
    "X_test_end_year    = 2018\n",
    "\n",
    "TRAIN_NUM_BLOCKIDS = TEST_NUM_BLOCKIDS = 800\n",
    "\n",
    "TRAIN_BLOCKIDS = random.sample(list(range(1,802)), k=TRAIN_NUM_BLOCKIDS)   \n",
    "train_blockid_dict = {}\n",
    "for ind, blockid in enumerate(TRAIN_BLOCKIDS ):\n",
    "    train_blockid_dict[blockid] = ind\n",
    "        \n",
    "TEST_BLOCKIDS = random.sample(list(range(1,802)), k=TEST_NUM_BLOCKIDS)    \n",
    "test_blockid_dict = {}\n",
    "for ind, blockid in enumerate(TEST_BLOCKIDS ):\n",
    "    test_blockid_dict[blockid] = ind\n",
    "\n",
    "X_train, X_test, y_train, y_test = ready_data(2015, 2017, train_blockid_dict,\n",
    "                                              X_test_start_year, X_test_end_year, test_blockid_dict)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_output(y, y_pred, dataset_type):\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    plt.plot(np.arange(len(y.flatten())),\n",
    "                 y.flatten(), color='blue');\n",
    "    plt.plot(np.arange(len(y_pred.flatten())),\n",
    "                 y_pred.flatten(), color='red');\n",
    "    plt.xlabel('Hour since beginning of data', fontsize=16)\n",
    "    plt.ylabel('Risk', fontsize=18)\n",
    "    plt.title(dataset_type + ' dataset', fontsize=18)\n",
    "    plt.legend(labels=['risk', 'predicted risk'], prop={'size': 20})\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_validation_loss(epochs, history):\n",
    "    plt.plot(range(1, epochs), history.history['loss'])\n",
    "    plt.plot(range(1, epochs), history.history['val_loss'])\n",
    "    plt.title('model train vs validation loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Reshape, Dropout, LeakyReLU, Bidirectional\n",
    "from keras.optimizers import Adam, SGD, RMSprop, Adagrad, Adadelta, Adamax, Nadam\n",
    "import keras.backend as K\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "random.seed(101)\n",
    "best_model = None\n",
    "best_mse   = None\n",
    "\n",
    "def perc_error(y_true, y_pred):\n",
    "    return 100.0 * K.mean((y_true - y_pred) / y_true)\n",
    "\n",
    "def create_model(learn_rate=0.01, \n",
    "                 momentum=0,\n",
    "                 opt_name='Adam',\n",
    "                 init_mode='uniform',\n",
    "                 activation='leakyrelu'\n",
    "                ):\n",
    "    data_dim    = 7*24+1   # All values in each hour of the week\n",
    "                           # averaged over each day for all weeks\n",
    "                           # of the month\n",
    "    timesteps   = 2 * 12   # Summed per month\n",
    "    batch_size  = 64\n",
    "    num_outputs = (7*24+1) * 12\n",
    "    \n",
    "    if activation == 'leakyrelu':\n",
    "        activation = LeakyReLU()\n",
    "    \n",
    "    # expected input data shape: (batch_size, timesteps, data_dim)\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Bidirectional(LSTM(2000, input_shape=(24,169), \n",
    "                   activation='relu', recurrent_activation='relu', \n",
    "                   dropout=0.2), merge_mode='ave'))\n",
    "    model.add(Dense(12*168, kernel_initializer=init_mode, activation='relu'))\n",
    "    model.add(Reshape((12,168)))\n",
    "    \n",
    "    optimizer = { \n",
    "        'SGD':      SGD(lr=learn_rate, momentum=momentum),\n",
    "        'RMSprop':  RMSprop(lr=learn_rate),\n",
    "        'Adagrad':  Adagrad(lr=learn_rate),\n",
    "        'Adadelta': Adadelta(lr=learn_rate),\n",
    "        'Adam':     Adam(lr=learn_rate),\n",
    "        'Adamax':   Adamax(lr=learn_rate),\n",
    "        'Nadam':    Nadam(lr=learn_rate),\n",
    "    }\n",
    "\n",
    "    def sqrt_loss(y_pred, y_true):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "    \n",
    "    model.compile(loss='mean_squared_error', metrics=[sqrt_loss], optimizer=optimizer[opt_name])\n",
    "\n",
    "    return model\n",
    "\n",
    "def my_gridsearch(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    # GridSearchCV's fit method requires X which is 2D, y which is 1D.\n",
    "    # This is a problem for us since our X and y are 3D.\n",
    "    # Instead of GridSearchCV, we will create our own loop to\n",
    "    # search through the grid.\n",
    "    \n",
    "    global best_model, best_mse\n",
    "    \n",
    "    # \n",
    "    #     for epochs in range(5, 6, 5):\n",
    "    #         for lr in [0.00001, 0.0001, 0.001, 0.01, 0.1]:\n",
    "    #             for opt_name in ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']:\n",
    "    #                 for init_mode in ['uniform', 'lecun_uniform', 'normal', 'zero',\n",
    "    #                                   'glorot_normal', 'glorot_uniform', 'he_normal',\n",
    "    #                                   'he_uniform']:\n",
    "    #                     for activation in ['relu']:\n",
    "    batch_size = 64\n",
    "    for epochs in range(5, 6, 5):\n",
    "        for lr in [1e-4]:\n",
    "            for opt_name in ['Adam']:\n",
    "                for init_mode in ['uniform', 'lecun_uniform', 'zero']:\n",
    "                    for activation in ['relu', 'leakyrelu']:\n",
    "\n",
    "                        print('>'*80)\n",
    "                        print('epochs:', epochs, '   lr:', lr, \\\n",
    "                              '   opt:', opt_name, \\\n",
    "                              '   init:', init_mode, \\\n",
    "                              '   act:', 'relu')\n",
    "                        print('>'*80)\n",
    "\n",
    "                        model = create_model(learn_rate=lr, \n",
    "                                             opt_name=opt_name,\n",
    "                                             init_mode=init_mode,\n",
    "                                             activation=activation)\n",
    "\n",
    "                        history = model.fit(X_train, y_train,\n",
    "                                            batch_size=batch_size, \n",
    "                                            epochs=epochs,\n",
    "                                            validation_data=(X_test, y_test))\n",
    "\n",
    "                        mse = model.evaluate(X_test, y_test,\n",
    "                                             batch_size=batch_size)\n",
    "                        if (best_mse is None) or \\\n",
    "                            (mse < best_mse):\n",
    "                            best_mse = mse\n",
    "                            best_model = model\n",
    "                            print('best_model:', best_model)\n",
    "                        \n",
    "                        print('<'*80)\n",
    "                        print('epochs:', epochs, \\\n",
    "                              '   lr:', lr, \\\n",
    "                              '   opt:', opt_name, \\\n",
    "                              '   init:', init_mode, \\\n",
    "                              '   act:', 'relu', \\\n",
    "                              ' Test MSE:', mse \\\n",
    "                        )\n",
    "                        print('<'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model = create_model(learn_rate=0.001)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-0de43f536de5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    675\u001b[0m             \u001b[0;31m# to match the value shapes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_set_inputs\u001b[0;34m(self, inputs, outputs, training)\u001b[0m\n\u001b[1;32m    587\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/sequential.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_input_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/layers/wrappers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBidirectional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0;31m# Applies the same workaround as in `RNN.__call__`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    429\u001b[0m                                          \u001b[0;34m'You can build it manually via: '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m                                          '`layer.build(batch_input_shape)`')\n\u001b[0;32m--> 431\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/layers/wrappers.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    567\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/layers/recurrent.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    491\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep_input_shape\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mconstants_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_input_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0;31m# set or validate state_spec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/layers/recurrent.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m   1872\u001b[0m             \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurrent_initializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1873\u001b[0m             \u001b[0mregularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurrent_regularizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1874\u001b[0;31m             constraint=self.recurrent_constraint)\n\u001b[0m\u001b[1;32m   1875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1876\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36madd_weight\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint)\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloatx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         weight = K.variable(initializer(shape),\n\u001b[0m\u001b[1;32m    250\u001b[0m                             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m                             \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/initializers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, shape, dtype)\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_matrices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0;31m# Pick the one with the correct shape.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mflat_shape\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36msvd\u001b[0;34m(a, full_matrices, compute_uv)\u001b[0m\n\u001b[1;32m   1560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m         \u001b[0msignature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'D->DdD'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'd->ddd'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1562\u001b[0;31m         \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgufunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m         \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_realType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=64, epochs=50, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    from sqlalchemy import create_engine, func, text\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sqlalchemy.orm import sessionmaker\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from decouple import config\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from shapely import wkb, wkt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from shapely.geometry import Point\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from geoalchemy2.shape import to_shape\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import random\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import json\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from datetime import datetime, timedelta\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import re\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from matplotlib import pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import RobustScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import LSTM, Dense\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import GridSearchCV\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import uniform\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sqlalchemy.ext.declarative import declarative_base\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sqlalchemy import Column, BigInteger, Integer, String, DateTime, ForeignKey, Float\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sqlalchemy.orm import relationship\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from geoalchemy2 import Geometry\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from contextlib import contextmanager\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import LSTM, Dense, Reshape, Dropout, LeakyReLU, Bidirectional\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.optimizers import Adam, SGD, RMSprop, Adagrad, Adadelta, Adamax, Nadam\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import keras.backend as K\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.wrappers.scikit_learn import KerasRegressor\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.datasets import mnist\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.core import Dense, Dropout, Activation\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.utils import np_utils\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from decouple import config\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'LSTM': hp.choice('LSTM', 256),\n",
      "        'dropout': hp.uniform('dropout', 0,1),\n",
      "        'optimizer': hp.choice('optimizer', ['adam']),\n",
      "        'batch_size': hp.choice('batch_size', [128]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "1: \n",
      "2: \n",
      "3: \n",
      "4: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "  1: def keras_fmin_fnct(space):\n",
      "  2: \n",
      "  3:     def rmse(y_true, y_pred):\n",
      "  4:         return K.sqrt(K.mean(K.square(y_true - y_pred)))\n",
      "  5: \n",
      "  6:     model = Sequential()\n",
      "  7:     model.add(LSTM(space['LSTM'],  # ,512,1024,2048,4096)}}, \n",
      "  8:                    input_shape=(24,169), activation=LeakyReLU(), recurrent_activation=LeakyReLU()), dropout=space['dropout'])\n",
      "  9:     model.add(Dense(12*168, activation='relu'))\n",
      " 10:     model.add(Reshape((12,168)))\n",
      " 11:     model.compile(loss='mean_squared_error', metrics=[rmse], optimizer=space['optimizer'])\n",
      " 12:     # , 'rmsprop', sgd'])}})\n",
      " 13: \n",
      " 14:     result = model.fit(x_train, y_train,\n",
      " 15:               batch_size=space['batch_size'],\n",
      " 16:               epochs=10,\n",
      " 17:               verbose=2,\n",
      " 18:               validation_split=0.1)\n",
      " 19: \n",
      " 20:     validation_rmse = np.amin(result.history['val_rmse']) \n",
      " 21:     print('Best validation rmse of epoch:', validation_rmse)\n",
      " 22:     return {'loss': validation_rmse, 'status': STATUS_OK, 'model': model}\n",
      " 23: \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'int' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-c6584912024e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m                                         \u001b[0mmax_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                                         \u001b[0mtrials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                                         notebook_name='Chicago_predictions')\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/hyperas/optim.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(model, data, algo, max_evals, trials, functions, rseed, notebook_name, verbose, eval_space, return_space, keep_temp)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                      \u001b[0mnotebook_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnotebook_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                                      \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                                      keep_temp=keep_temp)\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/hyperas/optim.py\u001b[0m in \u001b[0;36mbase_minimizer\u001b[0;34m(model, data, functions, algo, max_evals, trials, rseed, full_model_string, notebook_name, verbose, stack, keep_temp)\u001b[0m\n\u001b[1;32m    132\u001b[0m     return (\n\u001b[1;32m    133\u001b[0m         fmin(keras_fmin_fnct,\n\u001b[0;32m--> 134\u001b[0;31m              \u001b[0mspace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m              \u001b[0malgo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malgo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m              \u001b[0mmax_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_evals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SageMaker/LambdaLabs12/DS/temp_model.py\u001b[0m in \u001b[0;36mget_space\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/hyperopt/pyll_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(label, *args, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_real_string\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_literal_string\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'require string label'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/hyperopt/pyll_utils.py\u001b[0m in \u001b[0;36mhp_choice\u001b[0;34m(label, options)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mhp_choice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     ch = scope.hyperopt_param(label,\n\u001b[0;32m---> 59\u001b[0;31m                               scope.randint(len(options)))\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'int' has no len()"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from keras.datasets import mnist\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "\n",
    "\n",
    "def data():\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "def create_model(x_train, y_train, x_test, y_test):\n",
    "    def rmse(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_true - y_pred)))\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM({{choice(256)}},  # ,512,1024,2048,4096)}}, \n",
    "                   input_shape=(24,169), activation=LeakyReLU(), recurrent_activation=LeakyReLU()), dropout={{uniform(0,1)}})\n",
    "    model.add(Dense(12*168, activation='relu'))\n",
    "    model.add(Reshape((12,168)))\n",
    "    model.compile(loss='mean_squared_error', metrics=[rmse], optimizer={{choice(['adam'])}})\n",
    "    # , 'rmsprop', sgd'])}})\n",
    "\n",
    "    result = model.fit(x_train, y_train,\n",
    "              batch_size={{choice([128])}},\n",
    "              epochs=10,\n",
    "              verbose=2,\n",
    "              validation_split=0.1)\n",
    "\n",
    "    validation_rmse = np.amin(result.history['val_rmse']) \n",
    "    print('Best validation rmse of epoch:', validation_rmse)\n",
    "    return {'loss': validation_rmse, 'status': STATUS_OK, 'model': model}\n",
    "\n",
    "\n",
    "\n",
    "best_run, best_model = optim.minimize(model=create_model,\n",
    "                                        data=data,\n",
    "                                        algo=tpe.suggest,\n",
    "                                        max_evals=10,\n",
    "                                        trials=Trials(),\n",
    "                                        notebook_name='Chicago_predictions')\n",
    "X_train, Y_train, X_test, Y_test = data()\n",
    "print(best_model.evaluate(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas import optim\n",
    "\n",
    "best_run = optim.minimize(model=create_model,\n",
    "                          data=data,\n",
    "                          algo=tpe.suggest,\n",
    "                          max_evals=10,\n",
    "                          trials=Trials())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with session_scope() as SESSION:\n",
    "    pred_blockid_dict = {}\n",
    "    for i, x in enumerate(range(1,802)):\n",
    "        pred_blockid_dict[x] = i\n",
    "    X_pred = np.zeros((801, 24, 7*24+1))\n",
    "    for r in GetData().go(SESSION, 2017, 2018):\n",
    "        X_pred[pred_blockid_dict[r[0]], 12*(r[1]-2017)+r[2]-1, 24*r[3]+r[4]] = float(r[5])\n",
    "\n",
    "    for i in range(24):\n",
    "        X_pred[:, i, -1] = (2017*12+i) / (2000 * 12)\n",
    "    y_pred = model.predict(X_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "my_gridsearch(X_train, y_train, X_test, y_test)\n",
    "print('Best test MSE:', best_mse)\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = best_model.predict(X_train).flatten()\n",
    "y_test_pred = best_model.predict(X_test).flatten()\n",
    "\n",
    "plot_output(y_train, y_train_pred, 'Training')\n",
    "plot_output(y_test, y_test_pred, 'Testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_y_stats(s, y):\n",
    "    num_spaces = 2\n",
    "    if isinstance(y, pd.core.series.Series):\n",
    "        y_flat = y\n",
    "    elif isinstance(y, np.ndarray):\n",
    "        y_flat = y.flatten()\n",
    "    else:\n",
    "        raise ValueError('Could not process type:', type(y))\n",
    "        \n",
    "    print(s)\n",
    "    print(' ' * num_spaces, 'min: ', min(y_flat))\n",
    "    print(' ' * num_spaces, 'max: ', max(y_flat))\n",
    "    print(' ' * num_spaces, 'mean:', np.mean(y_flat))\n",
    "    print(' ' * num_spaces, 'std: ', np.std(y_flat))\n",
    "    return min(y_flat), max(y_flat), np.mean(y_flat), np.std(y_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, _, y_train_std = print_y_stats('y_train:', y_train)\n",
    "print()\n",
    "print_y_stats('y_test:', y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store predictions in DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decouple import config\n",
    "\n",
    "def store_predictions_in_db(y_pred):\n",
    "    \n",
    "    DB_URI_WRITE  = config('DB_URI_WRITE')\n",
    "\n",
    "    # Put predictions into pandas DataFrame with corresponding block id\n",
    "    predictions = pd.DataFrame([[x] for x in pred_blockid_dict.keys()], columns=[\"id\"])\n",
    "\n",
    "    predictions.loc[:, \"prediction\"] = predictions[\"id\"].apply(lambda x: y_pred[pred_blockid_dict[x],:,:].astype(np.float64).tobytes().hex())\n",
    "    predictions.loc[:, \"month\"] = 0\n",
    "    predictions.loc[:, \"year\"] = 2019\n",
    "    predictions.to_csv(\"predictions.csv\", index=False)\n",
    "\n",
    "    # Query SQL\n",
    "    query_commit_predictions = \"\"\"\n",
    "    CREATE TEMPORARY TABLE temp_predictions (\n",
    "        id SERIAL PRIMARY KEY,\n",
    "        prediction TEXT,\n",
    "        month INTEGER,\n",
    "        year INTEGER\n",
    "    );\n",
    "\n",
    "    COPY temp_predictions (id, prediction, month, year) FROM STDIN DELIMITER ',' CSV HEADER;\n",
    "\n",
    "    UPDATE block\n",
    "    SET \n",
    "        prediction = DECODE(temp_predictions.prediction, 'hex'),\n",
    "        month = temp_predictions.month,\n",
    "        year = temp_predictions.year \n",
    "    FROM temp_predictions\n",
    "    WHERE block.id = temp_predictions.id;\n",
    "\n",
    "    DROP TABLE temp_predictions;\n",
    "    \"\"\"\n",
    "\n",
    "    # Open saved predictions and send to database using above query\n",
    "    with open(\"predictions.csv\", \"r\") as f:\n",
    "        print(\"SENDING TO DB\")\n",
    "        RAW_CONN = create_engine(DB_URI_WRITE).raw_connection()\n",
    "        cursor = RAW_CONN.cursor()\n",
    "        cursor.copy_expert(query_commit_predictions, f)\n",
    "        RAW_CONN.commit()\n",
    "        RAW_CONN.close()\n",
    "\n",
    "    for r in SESSION.execute(\"SELECT ENCODE(prediction::BYTEA, 'hex'), id FROM block WHERE prediction IS NOT NULL LIMIT 5;\").fetchall():\n",
    "        print(np.frombuffer(bytes.fromhex(r[0]), dtype=np.float64).reshape((12,7,24)))\n",
    "        print(y_pred[pred_blockid_dict[int(r[1])], :].reshape((12,7,24)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_predictions_in_db(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try filtering values that are 20 standard deviations above the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = np.std(y_train)\n",
    "y_train_filtered = y_train[y_train < 20*std] # Remove all values larger than 20 standard deviations\n",
    "\n",
    "std = np.std(y_test)\n",
    "y_test_filtered = y_test[y_test < 20*std]   # Remove all values larger than 20 standard deviations\n",
    "\n",
    "print('Number of values filtered from y_train:', len(y_train[y_train > 20*std]))\n",
    "print('Number of values filtered from y_test:', len(y_test[y_test > 20*std]))\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "plt.plot(y_train_filtered, color='blue');\n",
    "plt.plot(y_test_filtered, color='red');\n",
    "plt.legend(labels=['training set data', 'testing set data'], prop={'size': 20})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With our threshold of 20 * std, we have removed 13 points from y_train and 12 from y_test. This is out of 1.6 million points, so they were defintely outliers. Let's run the prediction again with the updated y values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_updated = np.where(y_train.flatten() < 20*std, y_train.flatten(), [0.]*len(y_train.flatten()))\n",
    "y_test_updated  = np.where(y_test.flatten() < 20*std, y_test.flatten(), [0.]*len(y_test.flatten()))\n",
    "\n",
    "predict(X_train, \n",
    "        y_train_updated.reshape((801, 12, 168)), \n",
    "        X_test, \n",
    "        y_test_updated.reshape((801, 12, 168)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_updated_pred = model.predict(X_train).flatten()\n",
    "y_test_updated_pred = model.predict(X_test).flatten()\n",
    "\n",
    "plot_output(y_train_updated, y_train_updated_pred, 'Training')\n",
    "plot_output(y_test_updated, y_test_updated_pred, 'Testing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of y-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "pd.Series(y_train_updated).hist(bins=np.arange(0.0001, 0.002, 0.0002));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "pd.Series(y_test_updated).hist(bins=np.arange(0.0001, 0.002, 0.0002));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This data seems reasonable, although it is a little lopsided. Still, it shouldn't cause the neural network to give us the large error that it is giving. Let's try giving the network the full set of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the full set of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetDataFull(object):\n",
    "    def go(self, SESSION, start_year, end_year):\n",
    "        SQL_QUERY = \\\n",
    "            f'''\n",
    "            SELECT incident.blockid, \n",
    "                    incident.year, \n",
    "                    incident.month, \n",
    "                    incident.dow, \n",
    "                    incident.hour,\n",
    "                    SUM(crimetype.severity)/AVG(block.population) AS severity\n",
    "            FROM incident\n",
    "            INNER JOIN block ON incident.blockid = block.id INNER JOIN crimetype ON incident.crimetypeid = crimetype.id AND block.population > 0\n",
    "                AND block.population > 0\n",
    "                AND severity > 0\n",
    "                AND incident.cityid = 1\n",
    "                AND incident.year >= {start_year}\n",
    "                AND incident.year <= {end_year}\n",
    "            GROUP BY\n",
    "                incident.blockid,\n",
    "                incident.year,\n",
    "                incident.month,\n",
    "                incident.dow,\n",
    "                incident.hour\n",
    "            '''\n",
    "        return SESSION.execute(text(SQL_QUERY)).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def days_in_month(year, month):\n",
    "    p = pd.Period(f'{year}-{month}-1')\n",
    "    return p.days_in_month\n",
    "\n",
    "def day_of_week(dt):\n",
    "    return dt.weekday()\n",
    "\n",
    "def create_arrays(blockids, start_year, end_year):\n",
    "    idx = 0\n",
    "    X_blockid, X_year, X_month, X_dow, X_hour, X_risk = [], [], [], [], [], []\n",
    "    for blockid in blockids:\n",
    "        for year in range(start_year, end_year + 1):\n",
    "            for month in range(1, 12 + 1):      # month range is 1-12\n",
    "                for day in range(1, days_in_month(year, month) + 1):\n",
    "                    for hour in range(24):      # hour range is 0-23\n",
    "                        X_blockid.append(blockid)\n",
    "                        X_year.append(year)\n",
    "                        X_month.append(month)\n",
    "                        X_dow.append(day_of_week(datetime(year, month, day)))\n",
    "                        X_hour.append(hour)\n",
    "                        X_risk.append(0.0)\n",
    "                        idx += 1\n",
    "    \n",
    "    X = pd.DataFrame({'blockid':  X_blockid,\n",
    "                      'year':     X_year,\n",
    "                      'month':    X_month,\n",
    "                      'dow':      X_dow,\n",
    "                      'hour':     X_hour,\n",
    "                      'risk':     X_risk})\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_full(data, start_year, end_year):\n",
    "\n",
    "    def remove_outliers_from_risk(risk):\n",
    "        std = np.std(risk)\n",
    "        risk = np.where(risk < 20*std, \n",
    "                     risk, \n",
    "                     [0.]*len(risk)).reshape(risk.shape)\n",
    "\n",
    "        return risk\n",
    "    \n",
    "    NUM_BLOCKIDS = 801\n",
    "    \n",
    "    delta_years = end_year - start_year + 1\n",
    "    \n",
    "    blockid_dict = {}\n",
    "\n",
    "    # Create random array (BLOCKIDS) from 1-801 inclusive\n",
    "    # of length NUM_BLOCKIDS\n",
    "    BLOCKIDS = random.choices(list(range(1,802)), k=NUM_BLOCKIDS)\n",
    "    \n",
    "    for ind, blockid in enumerate(BLOCKIDS):\n",
    "        blockid_dict[blockid] = ind\n",
    "\n",
    "    blockids = list(blockid_dict.values())\n",
    "    X = create_arrays(blockids, start_year, end_year)\n",
    "\n",
    "    # records is the list of rows we get from the query with this order:\n",
    "    #   blockid, year, month, dow, hour, risk\n",
    "    #   month is from 1 - 12\n",
    "\n",
    "    X1 = []\n",
    "    for r in data:\n",
    "        if r[0] in blockid_dict:            \n",
    "            X1.append((r[0], r[1], r[2], r[3], r[4], r[5]))\n",
    "\n",
    "    X1 = pd.DataFrame(data=X1,\n",
    "                      columns=['blockid', 'year', 'month', 'dow', 'hour','risk2'])\n",
    "    X = pd.merge(X, X1, \n",
    "                 how='left',\n",
    "                 left_on=['blockid', 'year', 'month', 'dow', 'hour'],\n",
    "                 right_on=['blockid', 'year', 'month', 'dow', 'hour']\n",
    "                )\n",
    "    X['all_risk'] = X.risk.astype(float) + X.risk2.astype(float)\n",
    "    X = X.drop(columns=['risk', 'risk2']) \\\n",
    "         .rename(mapper={'all_risk': 'risk'}, axis=1)\n",
    "    \n",
    "    y = X['risk'].copy()\n",
    "    X = X.drop(columns=['risk']).copy()\n",
    "    y = remove_outliers_from_risk(y)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ready_data_full(training_start_year, training_end_year,\n",
    "                    testing_start_year, testing_end_year):\n",
    "    with session_scope() as session:\n",
    "        training_data = GetDataFull().go(session,\n",
    "                                         training_start_year,\n",
    "                                         training_end_year)\n",
    "        testing_data = GetDataFull().go(session,\n",
    "                                         testing_start_year,\n",
    "                                         testing_end_year)\n",
    "        X_train, y_train = process_data_full(training_data,\n",
    "                                             training_start_year, \n",
    "                                             training_end_year)\n",
    "        X_test, y_test = process_data_full(testing_data,\n",
    "                                           testing_start_year, \n",
    "                                           testing_end_year)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_train, X_test, y_train, y_test = ready_data_full(2015, 2016, 2017, 2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_full(X_train, y_train, X_test, y_test):\n",
    "#     data_dim    = 5 # 7 * 24   # All values in each hour of the week\n",
    "#     timesteps   = 2 * 12   # Summed per month\n",
    "#     batch_size  = 64\n",
    "#     num_outputs = 7 * 24 * 12\n",
    "\n",
    "#     # expected input data shape: (batch_size, timesteps, data_dim)\n",
    "#     model = Sequential()\n",
    "#     model.add(LSTM(1024, return_sequences=True, \n",
    "# #                    input_shape=(timesteps, # 24\n",
    "# #                                 data_dim), # 168\n",
    "#                    activation='relu',\n",
    "#                    kernel_initializer='random_uniform',\n",
    "#                    bias_initializer='zeros'\n",
    "#                   )\n",
    "#              )\n",
    "#     model.add(LSTM(1024, return_sequences=True, input_shape=(timesteps, data_dim)))\n",
    "#     model.add(LSTM(128, input_shape=(timesteps, data_dim), activation='relu'))\n",
    "#     model.add(Dense(num_outputs, activation='relu'))\n",
    "#     model.add(Reshape((12, 7 * 24)))\n",
    "\n",
    "#     model.compile(loss='mean_squared_error',\n",
    "#                    optimizer=Adam(lr=0.1))\n",
    "\n",
    "#     history = model.fit(X_train, y_train,\n",
    "#                         batch_size=batch_size, epochs=10,\n",
    "#                         validation_data=(X_test, y_test))\n",
    "\n",
    "#     mse = model.evaluate(X_test, y_test,\n",
    "#                          batch_size=batch_size)\n",
    "#     print('Test MSE:', mse)\n",
    "#     return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# history, model = predict_full(X_train, y_train, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
