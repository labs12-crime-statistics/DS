{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pip in c:\\users\\albeh\\anaconda3\\lib\\site-packages (19.1.1)\n",
      "Requirement already satisfied: python-decouple in c:\\users\\albeh\\anaconda3\\lib\\site-packages (3.1)\n",
      "Requirement already satisfied: geoalchemy2 in c:\\users\\albeh\\anaconda3\\lib\\site-packages (0.6.2)\n",
      "Requirement already satisfied: SQLAlchemy>=0.8 in c:\\users\\albeh\\anaconda3\\lib\\site-packages (from geoalchemy2) (1.3.2)\n",
      "Requirement already satisfied: shapely in c:\\users\\albeh\\anaconda3\\lib\\site-packages (1.6.4.post2)\n",
      "Requirement already satisfied: scipy in c:\\users\\albeh\\anaconda3\\lib\\site-packages (1.2.1)\n",
      "Requirement already satisfied: category-encoders in c:\\users\\albeh\\anaconda3\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: pandas>=0.21.1 in c:\\users\\albeh\\anaconda3\\lib\\site-packages (from category-encoders) (0.24.2)\n",
      "Requirement already satisfied: statsmodels>=0.6.1 in c:\\users\\albeh\\anaconda3\\lib\\site-packages (from category-encoders) (0.9.0)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\users\\albeh\\anaconda3\\lib\\site-packages (from category-encoders) (0.20.3)\n",
      "Requirement already satisfied: patsy>=0.4.1 in c:\\users\\albeh\\anaconda3\\lib\\site-packages (from category-encoders) (0.5.1)\n",
      "Requirement already satisfied: scipy>=0.19.0 in c:\\users\\albeh\\anaconda3\\lib\\site-packages (from category-encoders) (1.2.1)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\albeh\\anaconda3\\lib\\site-packages (from category-encoders) (1.16.2)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in c:\\users\\albeh\\anaconda3\\lib\\site-packages (from pandas>=0.21.1->category-encoders) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2011k in c:\\users\\albeh\\anaconda3\\lib\\site-packages (from pandas>=0.21.1->category-encoders) (2019.1)\n",
      "Requirement already satisfied: six in c:\\users\\albeh\\anaconda3\\lib\\site-packages (from patsy>=0.4.1->category-encoders) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install python-decouple\n",
    "!pip install geoalchemy2\n",
    "!pip install shapely\n",
    "!pip install scipy\n",
    "!pip install category-encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, func, text\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from decouple import config\n",
    "from shapely import wkb, wkt\n",
    "from shapely.geometry import Point\n",
    "from geoalchemy2.shape import to_shape \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, \\\n",
    "                            mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from category_encoders import BinaryEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tpot\n",
      "  Downloading https://files.pythonhosted.org/packages/15/c8/46f5c7231f8e3088052cda78ed36198f9ded9f5a5edfc99290f31aa6b57e/TPOT-0.10.1-py3-none-any.whl (74kB)\n",
      "Collecting update-checker>=0.16 (from tpot)\n",
      "  Downloading https://files.pythonhosted.org/packages/17/c9/ab11855af164d03be0ff4fddd4c46a5bd44799a9ecc1770e01a669c21168/update_checker-0.16-py2.py3-none-any.whl\n",
      "Requirement already satisfied: numpy>=1.12.1 in c:\\users\\albeh\\anaconda3\\lib\\site-packages (from tpot) (1.16.2)\n",
      "Requirement already satisfied: pandas>=0.20.2 in c:\\users\\albeh\\anaconda3\\lib\\site-packages (from tpot) (0.24.2)\n",
      "Requirement already satisfied: tqdm>=4.26.0 in c:\\users\\albeh\\anaconda3\\lib\\site-packages (from tpot) (4.31.1)\n",
      "Requirement already satisfied: scipy>=0.19.0 in c:\\users\\albeh\\anaconda3\\lib\\site-packages (from tpot) (1.2.1)\n",
      "Collecting stopit>=1.1.1 (from tpot)\n",
      "  Downloading https://files.pythonhosted.org/packages/35/58/e8bb0b0fb05baf07bbac1450c447d753da65f9701f551dca79823ce15d50/stopit-1.1.2.tar.gz\n",
      "Collecting deap>=1.0 (from tpot)\n",
      "  Downloading https://files.pythonhosted.org/packages/af/29/e7f2ecbe02997b16a768baed076f5fc4781d7057cd5d9adf7c94027845ba/deap-1.2.2.tar.gz (936kB)\n",
      "Requirement already satisfied: scikit-learn>=0.18.1 in c:\\users\\albeh\\anaconda3\\lib\\site-packages (from tpot) (0.20.3)\n",
      "Requirement already satisfied: requests>=2.3.0 in c:\\users\\albeh\\anaconda3\\lib\\site-packages (from update-checker>=0.16->tpot) (2.21.0)\n",
      "Requirement already satisfied: pytz>=2011k in c:\\users\\albeh\\anaconda3\\lib\\site-packages (from pandas>=0.20.2->tpot) (2019.1)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in c:\\users\\albeh\\anaconda3\\lib\\site-packages (from pandas>=0.20.2->tpot) (2.8.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\albeh\\anaconda3\\lib\\site-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\albeh\\anaconda3\\lib\\site-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (2019.3.9)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in c:\\users\\albeh\\anaconda3\\lib\\site-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (1.24.2)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\albeh\\anaconda3\\lib\\site-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (2.8)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\albeh\\anaconda3\\lib\\site-packages (from python-dateutil>=2.5.0->pandas>=0.20.2->tpot) (1.12.0)\n",
      "Building wheels for collected packages: stopit, deap\n",
      "  Building wheel for stopit (setup.py): started\n",
      "  Building wheel for stopit (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\albeh\\AppData\\Local\\pip\\Cache\\wheels\\3c\\85\\2b\\2580190404636bfc63e8de3dff629c03bb795021e1983a6cc7\n",
      "  Building wheel for deap (setup.py): started\n",
      "  Building wheel for deap (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\albeh\\AppData\\Local\\pip\\Cache\\wheels\\22\\ea\\bf\\dc7c8a2262025a0ab5da9ef02282c198be88902791ca0c6658\n",
      "Successfully built stopit deap\n",
      "Installing collected packages: update-checker, stopit, deap, tpot\n",
      "Successfully installed deap-1.2.2 stopit-1.1.2 tpot-0.10.1 update-checker-0.16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\albeh\\Anaconda3\\lib\\site-packages\\deap\\tools\\_hypervolume\\pyhv.py:33: ImportWarning: Falling back to the python version of hypervolume module. Expect this to be very slow.\n",
      "  \"module. Expect this to be very slow.\", ImportWarning)\n"
     ]
    }
   ],
   "source": [
    "!pip install tpot\n",
    "from tpot import TPOTRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Contains models for DB.\"\"\"\n",
    "\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy import Column, BigInteger, Integer, String, DateTime, ForeignKey, Float\n",
    "from sqlalchemy.orm import relationship\n",
    "from geoalchemy2 import Geometry\n",
    "\n",
    "\n",
    "BASE = declarative_base()\n",
    "\n",
    "\n",
    "class City(BASE):\n",
    "    \"\"\"City model for DB. Has information of cities.\"\"\"\n",
    "    __tablename__ = 'city'\n",
    "    id            = Column(BigInteger, primary_key=True)\n",
    "    city          = Column(String, unique=False, nullable=False)\n",
    "    state         = Column(String, unique=False, nullable=True)\n",
    "    country       = Column(String, unique=False, nullable=False)\n",
    "    location      = Column(Geometry(geometry_type='POINT'), nullable=False)\n",
    "    blocks        = relationship(\"Blocks\", back_populates=\"city\")\n",
    "    zipcodes      = relationship(\"ZipcodeGeom\", back_populates=\"city\")\n",
    "    incidents     = relationship(\"Incident\", back_populates=\"city\")\n",
    "\n",
    "\n",
    "class Blocks(BASE):\n",
    "    \"\"\"Block model for DB. Has information of city blocks for a related city\n",
    "        id.\"\"\"\n",
    "    __tablename__ = 'block'\n",
    "    id            = Column(BigInteger, primary_key=True)\n",
    "    cityid        = Column(BigInteger, ForeignKey('city.id'), nullable=False)\n",
    "    shape         = Column(Geometry(geometry_type='MULTIPOLYGON'), nullable=False)\n",
    "    population    = Column(Integer, nullable=False)\n",
    "    city          = relationship(\"City\", back_populates=\"blocks\")\n",
    "    incidents     = relationship(\"Incident\", back_populates=\"block\")\n",
    "\n",
    "class ZipcodeGeom(BASE):\n",
    "    \"\"\"Zipcode geometry model for DB. Has information of zipcodes and related\n",
    "        city id.\"\"\"\n",
    "    __tablename__ = 'zipcodegeom'\n",
    "    id            = Column(BigInteger, primary_key=True)\n",
    "    cityid        = Column(BigInteger, ForeignKey('city.id'), nullable=False)\n",
    "    zipcode       = Column(String, nullable=False, unique=True)\n",
    "    shape         = Column(Geometry(geometry_type='MULTIPOLYGON'), nullable=False)\n",
    "    city          = relationship(\"City\", back_populates=\"zipcodes\")\n",
    "\n",
    "class Incident(BASE):\n",
    "    \"\"\"Incident model for DB. Has information of a specific crime, including\n",
    "        where it took place, when it took place, and the type of crime that\n",
    "        occurred.\"\"\"\n",
    "    __tablename__ = 'incident'\n",
    "    id            = Column(BigInteger, primary_key=True)\n",
    "    crimetypeid   = Column(BigInteger, ForeignKey('crimetype.id'), nullable=False)\n",
    "    locdescid     = Column(BigInteger, ForeignKey('locdesctype.id'), nullable=False)\n",
    "    cityid        = Column(BigInteger, ForeignKey('city.id'), nullable=False)\n",
    "    blockid       = Column(BigInteger, ForeignKey('block.id'), nullable=False)\n",
    "    location      = Column(Geometry(geometry_type='POINT'), nullable=False)\n",
    "    datetime      = Column(DateTime, nullable=False)\n",
    "    hour          = Column(Integer, nullable=False)\n",
    "    dow           = Column(Integer, nullable=False)\n",
    "    month         = Column(Integer, nullable=False)\n",
    "    year          = Column(Integer, nullable=False)\n",
    "    city          = relationship(\"City\", back_populates=\"incidents\")\n",
    "    block         = relationship(\"Blocks\", back_populates=\"incidents\")\n",
    "    crimetype     = relationship(\"CrimeType\", back_populates=\"incidents\")\n",
    "    locationdesc  = relationship(\"LocationDescriptionType\", back_populates=\"incidents\")\n",
    "\n",
    "class CrimeType(BASE):\n",
    "    \"\"\"CrimeType model for DB. Has information of the types of crime, including\n",
    "        a general description and the numerical severity of the crime.\"\"\"\n",
    "    __tablename__ = 'crimetype'\n",
    "    id            = Column(BigInteger, primary_key=True)\n",
    "    category      = Column(String, unique=True, nullable=False)\n",
    "    severity      = Column(Integer, nullable=False)\n",
    "    incidents     = relationship(\"Incident\", back_populates=\"crimetype\")\n",
    "\n",
    "\n",
    "class LocationDescriptionType(BASE):\n",
    "    \"\"\"Location description model for DB. Has information on the type of\n",
    "        location that the crime took place.\"\"\"\n",
    "    __tablename__ = 'locdesctype'\n",
    "    id            = Column(BigInteger, primary_key=True)\n",
    "    key1          = Column(String, nullable=False)\n",
    "    key2          = Column(String, nullable=False)\n",
    "    key3          = Column(String, nullable=False)\n",
    "    incidents     = relationship(\"Incident\", back_populates=\"locationdesc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetDataFull(object):\n",
    "    def go(self, SESSION, start_year, end_year):\n",
    "        SQL_QUERY = \\\n",
    "            f'''\n",
    "            SELECT incident.blockid,\n",
    "                    incident.datetime,\n",
    "                    incident.year, \n",
    "                    incident.month, \n",
    "                    incident.dow, \n",
    "                    incident.hour,\n",
    "                    SUM(crimetype.severity)/AVG(block.population) AS severity\n",
    "            FROM incident\n",
    "            INNER JOIN block ON incident.blockid = block.id INNER JOIN crimetype ON incident.crimetypeid = crimetype.id AND block.population > 0\n",
    "                AND block.population > 0\n",
    "                AND severity > 0\n",
    "                AND incident.cityid = 1\n",
    "                AND incident.year >= {start_year}\n",
    "                AND incident.year <= {end_year}\n",
    "            GROUP BY\n",
    "                incident.blockid,\n",
    "                incident.datetime,\n",
    "                incident.year,\n",
    "                incident.month,\n",
    "                incident.dow,\n",
    "                incident.hour\n",
    "            '''\n",
    "        return SESSION.execute(text(SQL_QUERY)).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def days_in_month(year, month):\n",
    "    p = pd.Period(f'{year}-{month}-1')\n",
    "    return p.days_in_month\n",
    "\n",
    "def day_of_week(dt):\n",
    "    return dt.weekday()\n",
    "\n",
    "def create_arrays(blockids, start_year, end_year):\n",
    "    idx = 0\n",
    "    X_blockid, X_datetime, X_year, X_month, X_day, X_dow, X_hour, X_risk = [], [], [], [], [], [], [], []\n",
    "    for blockid in blockids:\n",
    "        for year in range(start_year, end_year + 1):\n",
    "            for month in range(1, 12 + 1):      # month range is 1-12\n",
    "                for day in range(1, days_in_month(year, month) + 1):\n",
    "                    for hour in range(24):      # hour range is 0-23\n",
    "                        X_blockid.append(blockid)\n",
    "                        X_datetime.append(datetime(year, month, day, hour, 0, 0, 0))\n",
    "                        X_year.append(year)\n",
    "                        X_month.append(month)\n",
    "                        X_day.append(day)\n",
    "                        X_dow.append(day_of_week(datetime(year, month, day)))\n",
    "                        X_hour.append(hour)\n",
    "                        X_risk.append(0.0)\n",
    "                        idx += 1\n",
    "    \n",
    "    X = pd.DataFrame({'blockid':  X_blockid,\n",
    "                      'datetime': X_datetime,\n",
    "                      'year':     X_year,\n",
    "                      'month':    X_month,\n",
    "                      'day':      X_day,\n",
    "                      'dow':      X_dow,\n",
    "                      'hour':     X_hour,\n",
    "                      'risk':     X_risk})\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_full(data, start_year, end_year):\n",
    "\n",
    "    def remove_outliers_from_risk(risk):\n",
    "        std = np.std(risk)\n",
    "        risk = np.where(risk < 20*std, \n",
    "                     risk, \n",
    "                     [0.]*len(risk)).reshape(risk.shape)\n",
    "\n",
    "        return risk\n",
    "    \n",
    "    def binary_encode_blockids(X):\n",
    "        encoded_blockids = pd.DataFrame(BinaryEncoder(cols=['blockid']) \\\n",
    "                                        .fit_transform(X))\n",
    "        \n",
    "        X = pd.concat([X, encoded_blockids], axis=1,\n",
    "                      names=['blockid' + str(i) for i in range(1, 801 +1)])\n",
    "        X = X.drop(columns=['blockid'])\n",
    "        return X\n",
    "    \n",
    "    NUM_BLOCKIDS = 801\n",
    "    \n",
    "    delta_years = end_year - start_year + 1\n",
    "    \n",
    "    X = create_arrays(range(1, NUM_BLOCKIDS + 1), start_year, end_year)\n",
    "    X = X.drop(columns=['year', 'month', 'day', 'dow', 'hour'], axis=1)\n",
    "\n",
    "    # records is the list of rows we get from the query with this order:\n",
    "    #   blockid, datetime, year, month, dow, hour, risk\n",
    "    #   month is from 1 - 12\n",
    "    \n",
    "    X1 = []\n",
    "    for r in data:\n",
    "        X1.append((r[0], r[1], r[6]))\n",
    "\n",
    "    X1 = pd.DataFrame(data=X1,\n",
    "                      columns=['blockid', 'datetime', 'risk2'])\n",
    "    X1['risk2'] = remove_outliers_from_risk(X1['risk2'].astype(float))\n",
    "    \n",
    "    return X1, X\n",
    "\n",
    "#     X = pd.merge(X, X1, \n",
    "#                  how='left',\n",
    "#                  left_on=['blockid', 'datetime'],\n",
    "#                  right_on=['blockid', 'datetime']\n",
    "#                 )\n",
    "#     X['all_risk'] = X.risk.astype(float) + X.risk2.astype(float)\n",
    "#     df = X.drop(columns=['risk', 'risk2']) \\\n",
    "#          .rename(mapper={'all_risk': 'risk'}, axis=1)\n",
    "    \n",
    "#     y = df['risk'].copy()\n",
    "#     df = df.drop(columns=['risk'])\n",
    "    \n",
    "#     y = remove_outliers_from_risk(y)\n",
    "#     df = binary_encode_blockids(df)\n",
    "#     df['risk'] = y\n",
    "#     df1 = df.iloc[:, :-2]\n",
    "#     df2 = df.iloc[:, -1]\n",
    "#     df = pd.concat([df1, df2], axis=1)\n",
    "    \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def session_scope():\n",
    "    \"\"\"Provide a transactional scope around a series of operations.\"\"\"\n",
    "\n",
    "    DB_URI  = config('DB_URI')\n",
    "    ENGINE  = create_engine(DB_URI)\n",
    "    Session = sessionmaker(bind=ENGINE)\n",
    "    SESSION = Session()\n",
    "    \n",
    "    try:\n",
    "        yield SESSION\n",
    "        SESSION.commit()\n",
    "    except:\n",
    "        SESSION.rollback()\n",
    "        raise\n",
    "    finally:\n",
    "        SESSION.close()\n",
    "\n",
    "\n",
    "def ready_data_full(training_start_year, training_end_year,\n",
    "                    testing_start_year, testing_end_year):\n",
    "    with session_scope() as session:\n",
    "        training_data = GetDataFull().go(session,\n",
    "                                         training_start_year,\n",
    "                                         training_end_year)\n",
    "        testing_data = GetDataFull().go(session,\n",
    "                                         testing_start_year,\n",
    "                                         testing_end_year)\n",
    "        train, train_generated = process_data_full(training_data,\n",
    "                                                 training_start_year, \n",
    "                                                 training_end_year)\n",
    "        test, test_generated = process_data_full(testing_data,\n",
    "                                               testing_start_year, \n",
    "                                               testing_end_year)\n",
    "\n",
    "    return train, test, train_generated, test_generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train, test, train_generated, test_generated = ready_data_full(2015, 2016, 2017, 2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((507148, 3), (508511, 3), (14052744, 3), (14033520, 3))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape, train_generated.shape, test_generated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>blockid</th>\n",
       "      <th>datetime</th>\n",
       "      <th>risk2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-01 00:00:00</td>\n",
       "      <td>0.000605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-01 02:00:00</td>\n",
       "      <td>0.000908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-02 10:30:00</td>\n",
       "      <td>0.000605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-03 09:40:00</td>\n",
       "      <td>0.000303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-03 22:30:00</td>\n",
       "      <td>0.001816</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   blockid            datetime     risk2\n",
       "0        1 2015-01-01 00:00:00  0.000605\n",
       "1        1 2015-01-01 02:00:00  0.000908\n",
       "2        1 2015-01-02 10:30:00  0.000605\n",
       "3        1 2015-01-03 09:40:00  0.000303\n",
       "4        1 2015-01-03 22:30:00  0.001816"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2 = train.set_index('datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>blockid</th>\n",
       "      <th>datetime</th>\n",
       "      <th>risk</th>\n",
       "      <th>risk2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-01 00:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-01 01:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-01 02:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-01 03:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-01 04:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   blockid            datetime  risk     risk2\n",
       "0        1 2015-01-01 00:00:00   0.0  0.000605\n",
       "1        1 2015-01-01 01:00:00   0.0       NaN\n",
       "2        1 2015-01-01 02:00:00   0.0  0.000908\n",
       "3        1 2015-01-01 03:00:00   0.0       NaN\n",
       "4        1 2015-01-01 04:00:00   0.0       NaN"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group = train2.groupby('blockid')\n",
    "# for k in group.groups.keys():\n",
    "#     g = group.get_group(k)\n",
    "g = group.get_group(1)\n",
    "g['risk2'].reset_index(drop=True)\n",
    "df2 = pd.merge(train_generated, g.reset_index(), \n",
    "                 how='left',\n",
    "                 left_on=['blockid', 'datetime'],\n",
    "                 right_on=['blockid', 'datetime']\n",
    "                )\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "blockid              int64\n",
       "datetime    datetime64[ns]\n",
       "risk               float64\n",
       "risk2              float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\albeh\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Passing datetime64-dtype data to TimedeltaIndex is deprecated, will raise a TypeError in a future version\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>blockid</th>\n",
       "      <th>datetime</th>\n",
       "      <th>risk</th>\n",
       "      <th>risk2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1420070400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1420074000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1420077600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1420081200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1420084800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   blockid    datetime  risk     risk2\n",
       "0        1  1420070400   0.0  0.000605\n",
       "1        1  1420074000   0.0       NaN\n",
       "2        1  1420077600   0.0  0.000908\n",
       "3        1  1420081200   0.0       NaN\n",
       "4        1  1420084800   0.0       NaN"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['datetime']= pd.to_timedelta(df2.datetime).dt.total_seconds().astype(int)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>blockid</th>\n",
       "      <th>datetime</th>\n",
       "      <th>risk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1420070400</td>\n",
       "      <td>0.000605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1420074000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1420077600</td>\n",
       "      <td>0.000908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1420081200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1420084800</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   blockid    datetime      risk\n",
       "0        1  1420070400  0.000605\n",
       "1        1  1420074000  0.000000\n",
       "2        1  1420077600  0.000908\n",
       "3        1  1420081200  0.000000\n",
       "4        1  1420084800  0.000000"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df2.fillna(value=0)\n",
    "df2['all_risk'] = df2['risk'] + df2['risk2']\n",
    "df2 = df2.drop(columns=['risk', 'risk2']) \\\n",
    "            .rename(mapper={'all_risk': 'risk'}, axis=1)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>blockid</th>\n",
       "      <th>datetime</th>\n",
       "      <th>risk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1420070400</td>\n",
       "      <td>0.000605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17544</th>\n",
       "      <td>2</td>\n",
       "      <td>1420070400</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35088</th>\n",
       "      <td>3</td>\n",
       "      <td>1420070400</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52632</th>\n",
       "      <td>4</td>\n",
       "      <td>1420070400</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70176</th>\n",
       "      <td>5</td>\n",
       "      <td>1420070400</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       blockid    datetime      risk\n",
       "0            1  1420070400  0.000605\n",
       "17544        2  1420070400  0.000000\n",
       "35088        3  1420070400  0.000000\n",
       "52632        4  1420070400  0.000000\n",
       "70176        5  1420070400  0.000000"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.sort_values(by=['datetime', 'blockid'], inplace=True, ascending=[True, True])\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (10539558, 2) y_train.shape: (10539558,)\n",
      "X_val.shape: (3513186, 2) y_val.shape: (3513186,)\n"
     ]
    }
   ],
   "source": [
    "def split_train_validate(df, target_name, test_fraction=0.25):\n",
    "\n",
    "    test_size = int(df.shape[0] * test_fraction)\n",
    "    df_train = df.iloc[:df.shape[0]-test_size,  :]\n",
    "    df_val   = df.iloc[ df.shape[0]-test_size:, :]\n",
    "    X_train  = df_train.drop(columns=[target_name])\n",
    "    y_train  = df_train[target_name]\n",
    "    X_val    = df_val.drop(columns=[target_name])\n",
    "    y_val    = df_val[target_name]\n",
    "\n",
    "    print('X_train.shape:', X_train.shape, 'y_train.shape:', y_train.shape)\n",
    "    print('X_val.shape:', X_val.shape, 'y_val.shape:', y_val.shape)\n",
    "    \n",
    "    return X_train, X_val, y_train, y_val\n",
    "\n",
    "X_train, X_val, y_train, y_val = split_train_validate(df2, \n",
    "                                                     'risk', \n",
    "                                                     0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average of the training y against validation y gives MAE: 6.012492017674962e-08\n",
      "Average of the training y against itself gives MAE: 5.438305972121914e-08\n"
     ]
    }
   ],
   "source": [
    "def baseline(y_train, y_val):\n",
    "    \n",
    "    mean_value_train = y_train.mean()\n",
    "    \n",
    "    y_pred = [abs(y - mean_value_train) for y in y_val]\n",
    "    mean_value_ypred = np.average(y_pred)\n",
    "    mae = np.average([abs(y - mean_value_ypred) for y in y_pred], axis = 0)\n",
    "    return mae\n",
    "\n",
    "print('Average of the training y against validation y gives MAE:', baseline(y_train, y_val))\n",
    "print('Average of the training y against itself gives MAE:', baseline(y_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0018159806295399517\n",
      "2.7912827272782475e-08\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(df2['risk'].max())\n",
    "print(df2['risk'].mean())\n",
    "print(df2['risk'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Optimization Progress', max=20, style=ProgressStyle(descriptiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method Booster.__del__ of <xgboost.core.Booster object at 0x000001C43063A748>>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\albeh\\Anaconda3\\lib\\site-packages\\xgboost\\core.py\", line 957, in __del__\n",
      "    _check_call(_LIB.XGBoosterFree(self.handle))\n",
      "stopit.utils.TimeoutException\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: -1.9946394071306894e-11\n",
      "\n",
      "Best pipeline: RandomForestRegressor(input_matrix, bootstrap=False, max_features=0.6000000000000001, min_samples_leaf=8, min_samples_split=15, n_estimators=100)\n",
      "-2.0363419573247403e-11\n",
      "Wall time: 1h 46min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tpot = TPOTRegressor(generations=1, population_size=10, verbosity=2)\n",
    "tpot.fit(X_train, y_train)\n",
    "print(tpot.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_predict = tpot.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE is 2.0363419573247403e-11\n",
      "RMSE is 4.512584577960551e-06\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "MSE = mean_squared_error(y_val, y_val_predict)\n",
    "RMSE = (np.sqrt(MSE))\n",
    "\n",
    "print('MSE is {}'.format(MSE))\n",
    "print('RMSE is {}'.format(RMSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {   \n",
    "    'learning_rate': [0.05],\n",
    "    'n_estimators':  [50],\n",
    "    'max_depth': [2],\n",
    "}\n",
    "gridsearch = GridSearchCV(XGBRegressor(),\n",
    "                          param_grid=param_grid, \n",
    "                          cv=3, n_jobs=-1,\n",
    "                          return_train_score=True, verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:  3.6min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:  3.6min finished\n",
      "C:\\Users\\albeh\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "C:\\Users\\albeh\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, importance_type='gain',\n",
       "       learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "       min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
       "       nthread=None, objective='reg:linear', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=True,\n",
       "       subsample=1),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'learning_rate': [0.05], 'n_estimators': [50], 'max_depth': [2]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=10)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, importance_type='gain',\n",
       "       learning_rate=0.05, max_delta_step=0, max_depth=2,\n",
       "       min_child_weight=1, missing=None, n_estimators=50, n_jobs=1,\n",
       "       nthread=None, objective='reg:linear', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=True,\n",
       "       subsample=1)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-74672773.87127508])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch.cv_results_['mean_train_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae: 0.03847244355477365\n",
      "rmse: 0.03847244381942293\n"
     ]
    }
   ],
   "source": [
    "y_pred_gs = gridsearch.predict(X_val)\n",
    "print('mae:', mean_absolute_error(y_val, y_pred_gs))\n",
    "print('rmse:', np.sqrt(mean_squared_error(y_val, y_pred_gs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting eli5\n",
      "  Downloading https://files.pythonhosted.org/packages/ee/2b/246db9e1c2d6f38e999daf0c4d5e54f36fbd0b937ffb13a34d32c2139403/eli5-0.8.2-py2.py3-none-any.whl (98kB)\n",
      "Requirement already satisfied: six in c:\\users\\albeh\\anaconda3\\lib\\site-packages (from eli5) (1.12.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\albeh\\anaconda3\\lib\\site-packages (from eli5) (2.10.1)\n",
      "Collecting graphviz (from eli5)\n",
      "  Downloading https://files.pythonhosted.org/packages/1f/e2/ef2581b5b86625657afd32030f90cf2717456c1d2b711ba074bf007c0f1a/graphviz-0.10.1-py2.py3-none-any.whl\n",
      "Collecting tabulate>=0.7.7 (from eli5)\n",
      "  Downloading https://files.pythonhosted.org/packages/c2/fd/202954b3f0eb896c53b7b6f07390851b1fd2ca84aa95880d7ae4f434c4ac/tabulate-0.8.3.tar.gz (46kB)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in c:\\users\\albeh\\anaconda3\\lib\\site-packages (from eli5) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.9.0 in c:\\users\\albeh\\anaconda3\\lib\\site-packages (from eli5) (1.16.2)\n",
      "Collecting typing (from eli5)\n",
      "  Downloading https://files.pythonhosted.org/packages/4a/bd/eee1157fc2d8514970b345d69cb9975dcd1e42cd7e61146ed841f6e68309/typing-3.6.6-py3-none-any.whl\n",
      "Requirement already satisfied: scipy in c:\\users\\albeh\\anaconda3\\lib\\site-packages (from eli5) (1.2.1)\n",
      "Requirement already satisfied: attrs>16.0.0 in c:\\users\\albeh\\anaconda3\\lib\\site-packages (from eli5) (19.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\albeh\\anaconda3\\lib\\site-packages (from jinja2->eli5) (1.1.1)\n",
      "Building wheels for collected packages: tabulate\n",
      "  Building wheel for tabulate (setup.py): started\n",
      "  Building wheel for tabulate (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\albeh\\AppData\\Local\\pip\\Cache\\wheels\\2b\\67\\89\\414471314a2d15de625d184d8be6d38a03ae1e983dbda91e84\n",
      "Successfully built tabulate\n",
      "Installing collected packages: graphviz, tabulate, typing, eli5\n",
      "Successfully installed eli5-0.8.2 graphviz-0.10.1 tabulate-0.8.3 typing-3.6.6\n"
     ]
    }
   ],
   "source": [
    "!pip install eli5\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor, GradientBoostingRegressor)\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LRmodel = LinearRegression()\n",
    "LRmodel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae: 7.760622782725684e-08\n",
      "rmse: 4.512288868211553e-06\n"
     ]
    }
   ],
   "source": [
    "y_predLR = LRmodel.predict(X_val)\n",
    "print('mae:', mean_absolute_error(y_val, y_predLR))\n",
    "print('rmse:', np.sqrt(mean_squared_error(y_val, y_predLR)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime    1.933405e-16\n",
       "blockid    -2.034532e-10\n",
       "dtype: float64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(LRmodel.coef_,\n",
    "          X_train.columns).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFmodel = RandomForestRegressor(\n",
    "    n_estimators=100, \n",
    "    min_samples_leaf=0.004,\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=0.004, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RFmodel.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae: 5.713708899555593e-08\n",
      "rmse: 4.512584657087893e-06\n"
     ]
    }
   ],
   "source": [
    "y_predRF = RFmodel.predict(X_val)\n",
    "print('mae:', mean_absolute_error(y_val, y_predRF))\n",
    "print('rmse:', np.sqrt(mean_squared_error(y_val, y_predRF)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime    0.0\n",
       "blockid     0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(RFmodel.feature_importances_, \n",
    "          X_train.columns).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PermutationImportance(cv='prefit',\n",
       "           estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=0.004, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       "           n_iter=1, random_state=None, refit=True,\n",
       "           scoring='neg_mean_squared_error')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "permuter = PermutationImportance(RFmodel, scoring='neg_mean_squared_error', n_iter=1, cv='prefit')\n",
    "permuter.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\albeh\\Anaconda3\\lib\\site-packages\\eli5\\formatters\\html.py:234: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  rel_weight = (abs(weight) / weight_range) ** 0.7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    table.eli5-weights tr:hover {\n",
       "        filter: brightness(85%);\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        <table class=\"eli5-weights eli5-feature-importances\" style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto;\">\n",
       "    <thead>\n",
       "    <tr style=\"border: none;\">\n",
       "        <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">Weight</th>\n",
       "        <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "    </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(0, 100.00%, nan%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0\n",
       "                \n",
       "                    &plusmn; 0.0000\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                datetime\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr style=\"background-color: hsl(0, 100.00%, nan%); border: none;\">\n",
       "            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "                0\n",
       "                \n",
       "                    &plusmn; 0.0000\n",
       "                \n",
       "            </td>\n",
       "            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "                blockid\n",
       "            </td>\n",
       "        </tr>\n",
       "    \n",
       "    \n",
       "    </tbody>\n",
       "</table>\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eli5.show_weights(permuter, top=None, feature_names=X_train.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=100, n_iter_no_change=None, presort='auto',\n",
       "             random_state=None, subsample=1.0, tol=0.0001,\n",
       "             validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GBmodel = GradientBoostingRegressor()\n",
    "GBmodel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae: 5.7258836731097064e-08\n",
      "rmse: 4.5125845779605515e-06\n"
     ]
    }
   ],
   "source": [
    "y_predGB = GBmodel.predict(X_val)\n",
    "print('mae:', mean_absolute_error(y_val, y_predGB))\n",
    "print('rmse:', np.sqrt(mean_squared_error(y_val, y_predGB)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
